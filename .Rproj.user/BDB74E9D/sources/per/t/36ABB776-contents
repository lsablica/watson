\documentclass[a4paper]{article}
\usepackage{amsfonts,amsthm}
\usepackage{amsmath, enumerate, bm}
\usepackage[round]{natbib}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{bm}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\P}{\mathbb{P}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{prop}{Proposition}
\newcommand{\pkg}[1]{\textbf{\textsf{#1}}}
\newcommand{\class}[1]{\textmd{\textsl{#1}}}
\newcommand{\fun}[1]{\texttt{#1}}
\DeclareMathOperator*{\argmax}{arg\,max} 
\def\code#1{\texttt{#1}}

\newcommand*{\addheight}[2][.5ex]{%
  \raisebox{0pt}[\dimexpr\height+(#1)\relax]{#2}%
}
\title{watson: An R Package for Fitting Mixtures of Watson Distributions}

<<include=FALSE>>=
#setwd('/home/sablica/Documents/Polytomous/writeup')
Sys.setenv(TEXINPUTS=getwd(),
           BIBINPUTS=getwd(),
           BSTINPUTS=getwd())
@

\begin{document}

\maketitle
\section{Introduction}

In the history of directional statistics the clustering on the sphere has gained a lot of popularity, while still being a difficult task mostly when it comes to estimation. The estimation troubles usually come from the difficulty to evaluate normalizing constant associated with the given directional distribution. These normalizing terms can be for many of the most used directional distributions represented in terms of infinite hypergeometric series, which not allow to obtain any closed or easy to evaluate form.

Probably the most natural choice of the directional distribution is the von Mises-Fisher (vMF) distribution.  The
vMF distribution has concentric contour lines similar to the multivariate normal distribution which allows to model data concentrated around these directions. Here the estimating procedure has been solved by introducing tight bounds on the normalizing constant  \citep{watson::Hornik2014E} that allowed to build a fitting framework for mixtures of such distributions \citep{watson::Hornik2014P}.  

While the vMF distribution is the first choice for most of the directional data, the distribution can be inappropriate if the data contain some additional structure. An example of such data are the axially symmetric data (i.e., $x$ and $-x$ are indistinguishable) used for example in structural geology or rock magnetism areas. An essential choice in these cases is the Watson distribution \citep{watson::Watson1965} whose mixture modelling will be of the main focus in this paper.

Recently, mostly thanks to the renewed attention in directional data modeling due to machine learning and sentiment analysis, the Watson distribution and its application in mixture modelling was discussed in \cite{watson:Bijral+Breitenbach+Grudic:2007} and  \cite{watson:Sra+Karp:2013}. The approximation of the maximum likelihood estimation using continued fractions shows anomalous convergence for a major set of values producing the loss of numerical precision, for more details see \cite{watson:Gautschi:1977}. On the other hand, the authors in \cite{watson:Bijral+Breitenbach+Grudic:2007} derived a maximum likelihood estimation using useful approximations but without any mathematical justification. A brilliant and mathematically justified approach is to use the derived bounds from  \cite{watson:Sra+Karp:2013} to approximate the true values during the estimation, which however again comes with the price of an approximation error. 

A new wind to this problematic was presented in \cite{watson::writeup1}, where the authors derived a convergent sequence of bounds that can be used during the estimation procedure. These bounds only after two iterations provide state of the art boundaries and allow to estimate the likelihood with arbitrary precision. This and also other numerical techniques and applications from the above paper will be used and adjusted to the modelling of the axial data using Watson distribution which is offered as an R-package \pkg{watson}. 


\section{The Watson distribution}
Following the notation in \cite{watson:mardia:2009}, let $\mathbb{S}^{p-1} = \{x\in\mathbb{R}^{p},\left \| x \right \|=1 \}$ be an $(p-1)$-dimensional unit sphere with the projective hyperplane $\mathbb{P}^{p-1}$ such that the symmetric vectors $\pm x\in\mathbb{S}^{p-1}$ are equivalent. A random unit length vector in $\mathbb{R}^p$ has a Watson distribution with concentration parameter $\kappa \in \mathbb{R}$ and mean direction parameter $\mu \in \mathbb{S}^{p-1}$, if its density with respect to the uniform distribution on the unit sphere is
$$f(x|\kappa ,\mu)=M\left (\frac{1}{2},\frac{p}{2},\kappa  \right )^{-1}e^{\kappa\left ( x^{t}\mu \right )^2}$$
where
$$M(a, b, z) ={{}_1F_1}(a; b; z) = \sum_{n=0}^\infty \frac{(a)_n}{(b)_n}\frac{z^n}{n!} $$
is the confluent hypergeometric function of the first kind, also known as Kummer's function, solving the Kummer's differential equation 
$z w'' + (b - z) w' - a w = 0.$ 

We note that if $\kappa>0$ the density has maxima at $\pm\mu$ and so is bipolar. On the other hand, for $\kappa<0$ the distribution is concentrated around the great circle orthogonal to $\mu$, and so the distribution is a symmetric girdle distribution. As $\kappa$ goes to $\pm\infty$ the distribution becomes more and more concentrated around $\mu$ if $\kappa\rightarrow\infty$ and around the orthogonal circle if $\kappa\rightarrow-\infty$. Finally, observe that for orthogonal $Q$ s.t. $Q\mu=\mu$, it holds that $\mu^{t}(Qx)=\mu^{t}x$, implying the rotational symmetry about $\mu$. For more details, see \cite{watson:mardia:2009}.

\subsection{Simulation of the Watson distribution}

\subsubsection{Algorithm structure}
We are interested in a random sample generation from a Watson distribution. For the following denote the Watson distribution as 
$$ f_{wat}(x|\kappa ,\mu)=c_{wat}f_{wat}^{*}(x|\kappa ,\mu)=c_{wat}e^{\kappa\left ( x^{t}\mu \right )^2},$$
where $c_{wat}$ is the normalizing constant. %For more details see \cite{watson:mardia:2009}. 
For the purpose of clarity, the sampling procedure will be split and investigated in three mutually exclusive cases.

Firstly, assume $\kappa=0$. The Watson distribution then simplifies to the uniform or isotropic distribution on sphere $S^{p}$ \citep{watson:bingham:1974}, which can be easily generated using $p$ standard normal random variables. Assume $X_{i}\sim N(0,1)$ and $X =(X_{1},\dots,X_{n})$, then $Y = X/\left \| X \right \|$ has a uniform distribution on a sphere $S$. To see this note that $X\sim N_{n}(0,I_{n})$ is invariant under rotations, i.e., $QX\stackrel{d}{=}X$ for any orthogonal $Q$, and since $\left \| Y \right \|=1$, $Y$ must be uniformly distributed on the sphere. 

For the cases of non-zero $\kappa$, the current literature that considers directly the Watson distribution is, according to our best knowledge, equipped only with the cases where $p\leq 3$, for example see \cite{watson:Best:1986} or \cite{watson:hung:1993}. Interestingly however, a slightly more general answer to the question was published by \cite{watson:Kent}, where the authors derived a BACG rejection sampling method for the Bingham distribution which generalizes the Watson one. As will be shown later, applying this theory directly only to the Watson distribution not only provides a sampling algorithm, but also allows to calculate the optimal hyper-parameters analytically, which is not the case for the general Bingham distribution.

The Bingham distribution is a symmetric probability distribution mainly used for modelling of axial data on the sphere $S^{p}$. It is a generalization of the Watson distribution and a special case of the Kent and Fisher-Bingham distributions. The density function is of the form $$f_{bing}(x|A)=c_{bing}f_{bing}^{*}(x|A)=c_{bing}e^{ -x^{t}Ax }$$ which can be obtained by the Watson distribution as $$ f_{Wat}(x|\kappa ,mu)=c_{Wat}e^{\kappa\left ( x^{t}\mu \right )^2}=c_{Wat}e^{x^{t}\overbrace{(-\kappa\mu\mu^t)}^{A}x}.$$ The minus in the exponential is not necessary but it allows simplifications in the further derivations. Additionally, recall that the Bingham distribution with parameter matrix $A+cI_{p}$ defines the same distribution for any $c\in\mathbb{R}$. This allows without any loss of generality to assume that the eigenvalues of $A$ satisfy $\min_{i=0,\dots,p}\lambda_{i}=0$, which will be also assumed in the upcoming steps.

Following \cite{watson:Kent}, let $b>0$ and define  $u=x^{t}Ax$ and $\Omega = I+2A/b,$ then the envelope inequality for the Bingham distribution is 
$$ f_{Bing}^{*}(x)\leq e^{-(q-b)/2}(q/b)^{q/2}f_{ACG}^{*}(x),$$
where $f_{ACG}^{*}(x)=\left(x^{t}\Omega x \right)^{-q/2}$ is the unnormalized density of angular central Gaussian distribution. Recall that if $Y\sim N_{p}(0,\Sigma)$ then $Y/\left \| Y \right \|\sim ACG(\Omega)$ with $\Omega=\Sigma^{-1}$. Thus, by simulating $U\sim U(0,1),$ $X\sim N_{p}(0,\Sigma)$, defining $Y=X/\left \| X \right \|$ we accept $Y$ if 
$$U <\frac{f_{Bing}^{*}\left (Y  \right )}{M\left ( b \right )f_{ACG}^{*}\left ( Y \right )}=\frac{e^{ -y^{t}Ay } }{e^{-(q-b)/2}(q/b)^{q/2}\left(y^{t}\Omega y \right)^{-q/2}}, $$
or equivalently, if
\begin{equation} \label{rejection}
\log(U) <  -y^{t}Ay +(q-b)/2 -(q/2)\log(q/b) +(q/2)\log\left(y^{t}\Omega y \right).
\end{equation}

Applying the theory to the Watson distribution, first assume a negative $\kappa$. The following procedure generates random vector from Watson distribution with $\kappa<0$ and $\mu=\left(0,0,\dots,1\right)$. It is easy to observe that the parameter matrix $A$ is then of the form
$$A=\begin{pmatrix}
0 & \hdots &0 \\ 
\vdots & \ddots & \vdots\\ 
0 &\hdots  & -\kappa
\end{pmatrix}$$ and so satisfies the assumption of non-negative eigenvalues. From this we can conclude that $$ 
\Sigma =\begin{pmatrix}
1 &0  & \hdots  \\ 
0 & \ddots & \vdots \\ 
\vdots & \hdots &b/(b-2\kappa)  
\end{pmatrix}.
$$ Thus, by simulating $p-1$ random variables $X_{i}\sim N(0,1)$ for $i=1,\dots,p-1,$ and one $X_{p}\sim N(0,b/(b-2\kappa))$, $X=\left(X_{1},X_{2},\dots,X_{p} \right)$ and $Y=X/\left \| X \right \|$. Since $A$ contains only one non-zero element and $\Omega$ is diagonal, also the expression \ref{rejection} simplifies significantly.

The authors in \cite{watson:Kent} state a unique efficiency maximizer for the parameter $b$ given as a solution of $$ 
\sum_{i=1}^{p}\frac{1}{b+2\lambda_{i}}=1,$$ where $\lambda_{i}$ is the $i$-th eigenvalue of $A$. Since only one eigenvalue is non-zero this can be written as $$1= \frac{\left (p-1  \right )}{b}+\frac{1}{b-2\kappa}=\frac{-2\left (p-1  \right )\kappa+pb}{b\left ( b-2\kappa \right )}$$ which gives the quadratic equation $b^2+b\left ( -2\kappa-p \right )+2\left (p-1  \right )\kappa=0$ with the larger root equal to $$
b=\frac{1}{2}\left ( p+2\kappa+\sqrt{p^2+4\left ( \kappa+2 \right )\kappa -4p\kappa} \right ) .$$ Clearly, the expression in the square root and solution are non-negative if $p\geq 1$ which is always the case.

Assuming now $\kappa>0$ and $\mu=\left(0,0,\dots,1\right)$, using the invariance under spherical shift to obtain positive eigenvalues $A$ and $\Sigma$ can be written as $$ 
A=\begin{pmatrix}
\kappa & 0&\hdots &0 \\
0 & \ddots& \ddots& \vdots\\  
\vdots & \ddots & \ddots&\vdots\\ 
0 &\hdots  & \hdots& 0
\end{pmatrix} \quad \text{and} \quad \Sigma=\begin{pmatrix}
b/(b+2\kappa)  & 0&\hdots &0 \\
0 & \ddots& \ddots& \vdots\\  
\vdots & \ddots & \ddots&\vdots\\ 
0 &\hdots  & \hdots& 1
\end{pmatrix}.
$$ This again simplifies the  expression \ref{rejection} as the required matrices are diagonal. The optimal $b$ can be again obtained implicitly as a solution of $b^2+b\left ( 2\kappa-p \right )-2\kappa=0,$ which has the larger root equal to $$b=\frac{1}{2}\left ( p-2\kappa+\sqrt{p^2+4 \kappa^{2} -4\kappa\left (p-2  \right )} \right ) .$$ Again the square root and $b$ are non-negative if $p\geq 1$.

To get samples for an arbitrary direction parameter $\mu$, the solution is multiplied
from the left with a $p\times p$ matrix  that consists of the first $(p-1)$ columns forming a unitary basis of
the subspace orthogonal to $\mu$, obtainable via QR decomposition, and the last column is equal to $\mu$. 

\subsubsection{Software}
The software implementation of the above theory can be obtained using the \fun{rmwat()} function from the \pkg{watson} package. 
<<echo=FALSE>>=
cat("R> rmwat(n, weights, kappa, mu, b = -10)")
@ 
The arguments for this function are as follows.
\begin{description}
\item[\normalfont \code{x}:] an integer giving the number of samples to draw.
\item[\normalfont \code{weights}:] a numeric vector with non-negative elements giving the mixture probabilities.
\item[\normalfont \code{kappa}:] a numeric vector giving the kappa parameters of the mixture components.
\item[\normalfont \code{mu}:] a numeric matrix with columns giving the mu parameters of the mixture components.
\item[\normalfont \code{b}:] a positive numeric hyper-parameter used in the sampling. If non-positive value is given, optimal choice of b is used, default: -10.
\end{description}

\begin{figure}[ht]
\includegraphics[scale=0.23]{sphere1.png}%
\includegraphics[scale=0.23]{sphere2.png}\\
\centerline{\footnotesize\hbox to 0.5\textwidth{\hfil
(a) $\kappa = (20,20)$ \hfil}\hbox to 0.5\textwidth{\hfil
(b) $\kappa = (-200,-200)$ \hfil}}
\caption{Resulting samples}
\end{figure}
\noindent
% {\bf Warning:} Due to the speed of simulation, the underlying C++ code does not contain any checks that would interrupt the simulation if the user would try to do so. This is in general safe as with the optimal parameter $b$ (derived above) the acceptance probability is close to $100\%$ (and increases with dimension). However, different choices of $b$ may cause difficulties in acceptance and are not recommended mostly for big dimensions unless you know what you are doing! The optimal $b$ is used if the input value is not suitable, i.e., is not positive (also the default case).
\newpage

\subsection{Estimation of the Watson distribution} \label{sec2}
Let $x_{1},\dots,x_{n}\in \mathbb{P}^{p-1}$ be i.i.d. sample from the Watson distribution with parameters $\kappa$ and $\mu$ and define ${\bf X}$ to be the design matrix with $x_{1},\dots,x_{n}$ as rows. The log-likelihood is then given by 
\begin{equation} \label{loglik}
\ell\left ( \kappa,\mu|x_{1}\dots x_{n} \right )= n(\kappa\mu^{t}{\bf S}\mu-\log\left ( M(1/2,p/2,\kappa) \right )+c),
\end{equation}
where ${\bf S}$ is the scatter matrix ${\bf S}={\bf X}^{t}{\bf X}/n $  and $c$ is a constant term that can be omitted. Since ${\bf S}$ is symmetric, $\mu^{t}{\bf S}\mu = R({\bf S},\mu)$ is a Rayleigh quotient of matrix ${\bf S}$ and thus satisfies $\lambda_{1}\leq R({\bf S},\mu) \leq \lambda_{p}, \forall \mu \in \mathbb{S}^{p-1}$, where $\lambda_{1}\leq\lambda_{2}\dots\leq\lambda_{p}$ are the ordered eigenvalues of ${\bf S}$. More precisely, it holds that $R({\bf S},s_{p})=\lambda_{p}$ and $R({\bf S},s_{1})=\lambda_{1}$, where $s_{1},\dots, s_{p}$ are the normalized eigenvectors corresponding to $\lambda_{1},\dots,\lambda_{p}$. Hence, maximizing \ref{loglik} with respect to $\mu$ gives obviously
\begin{equation} \label{mumax}
\hat{\mu}= \left\{\begin{matrix}
s_{1}, & \text{if } \hat{\kappa}<0, \\ 
s_{p}, & \text{if } \hat{\kappa}>0.\\ 
\end{matrix}\right.
\end{equation}
Taking the first order derivative with respect to $\kappa$ gives 
\begin{equation*} 
g\left(\frac{1}{2},\frac{p}{2}, \hat{\kappa}\right)=\frac{M'(\frac{1}{2},\frac{p}{2},\hat{\kappa})}{M(\frac{1}{2},\frac{p}{2},\hat{\kappa})}=\mu^{t}{\bf S}\mu=R({\bf S},\mu)=r,
\end{equation*}
with $0\leq \lambda_{1}\leq r\leq \lambda_{p}\leq 1, \ \forall \mu \in \mathbb{S}^{p-1} $ and $$g(a, b, \kappa) = \frac{\mathrm{d} \log(M(a, b,  \kappa)))}{\mathrm{d}  \kappa} =  \frac{{M(a, b,  \kappa)}'}{M(a, b,  \kappa)} = \frac{a}{b}\frac{{M(a+1, b+1,  \kappa)}}{M(a, b,  \kappa)},$$ as defined in \cite{watson:Sra+Karp:2013} or \cite{watson::writeup1}, yields the problem that has to be solved. Clearly, the maximizations are not independent, however, if $\hat{\kappa}>0,$ $s_{p}$ and $\lambda_{p}$ are optimal and if $\hat{\kappa}<0$, the solution is given by $s_{1}$ and $\lambda_{1}$. Thus, one can solve $g\left(\frac{1}{2},\frac{p}{2}, \hat{\kappa}\right)=\lambda_{1}$ and $g\left(\frac{1}{2},\frac{p}{2}, \hat{\kappa}\right)=\lambda_{2}$ and pick the combination with higher likelihood. 

To sum up, we are interested in the solution $\kappa$ of the highly-nonlinear problem 
\begin{equation} \label{kappasol}
g\left(a,b, \kappa\right)=r, \quad \text{where} \quad 0<a<b, \text{ and } 0\leq r\leq 1.
\end{equation}
It can be shown (see \cite{watson:Sra+Karp:2013}) that $g(a, b, z)$, for the case of Watson distribution, is a strictly increasing function which maps the interval $(-\infty,\infty)$ onto the interval $(0, 1)$. Even though $g(a, b, z)$ admits a continued fraction representation, possible approximation using continued fractions shows anomalous convergence for a major set of values producing the loss of numerical precision, for more details see \cite{watson:Gautschi:1977}.

\cite{watson:Sra:2007} suggested the ad-hoc approximation
\begin{equation} \label{sra2007}
\kappa \approx \left ( a +b -1 \right )\left ( \frac{1}{1-r}-\frac{a }{(b -1)r} \right )
\end{equation}
based on the approximation of $M(a,b,z)\approx M(a,b+1,z)$. 

\cite{watson:Bijral+Breitenbach+Grudic:2007} in around the same time offered another approximation, 
\begin{equation} \label{BBG}
\kappa \approx \frac{b r-a  }{r\left ( 1-r \right )},
\end{equation}
which they observed to be accurate for the Watson case. The equation was derived from the assumption $g(a,b,\kappa)\approx g(a+1,b+1,\kappa)$ and was also offered together with a correction term (``determined empirically"). The final form was presented as
\begin{equation} \label{BBG_c}
\kappa \approx \frac{b r-a  }{r\left ( 1-r \right )}+\frac{r}{2b\left ( 1-r \right )}.
\end{equation}

A huge step further was then accomplished in \cite{watson:Sra+Karp:2013}, where the authors derived tight bounds for the inverse of $g(a, b, \kappa)$. Defining 
\begin{align} \label{bounds}
\begin{split}
L(r)=&\frac{rb-a}{r(1-r)}\left ( 1+\frac{1-r}{b-a} \right ),\\
B(r)=&\frac{rb-a}{2r(1-r)}\left ( 1+\sqrt{1+\frac{4\left ( b +1 \right )r\left ( 1-r \right )}{a\left (b-a  \right )}} \right ),\\
U(r)=&\frac{rb-a}{r(1-r)}\left ( 1+\frac{r}{a} \right ),
\end{split}
\end{align}
it has been shown that the solution of equation \ref{kappasol} satisfies
\begin{align*} 
\begin{split}
&L(r)<\kappa<B(r)<U(r) \quad \text{for }0<r<a/b, \\
&L(r)<B(r)<\kappa<U(r) \quad \text{for }a/b<r<1,
\end{split}
\end{align*}
where all bounds are additionally exact at $\kappa = 0$, i.e., $r=a/b$. To have a unanimous decision rule which bound to use, the authors suggest the following rule-of-thumb:
\begin{equation} \label{sra2013}
\kappa \approx \left\{\begin{matrix}
U(r), & \text{if}& 0<r<\frac{a}{2b},\\ 
B(r), & \text{if}& \frac{a}{2b}\leq r<\frac{2a}{\sqrt{b}},\\ 
L(r), & \text{if}& \frac{2a}{\sqrt{b}}\leq r<1.
\end{matrix}\right. \\
\end{equation}
Furthermore, \cite{watson:Sra+Karp:2013} introduced a closed form Newton algorithm to solve \ref{kappasol}, given the assumption that  $g(a, b, \kappa)$ can be easily evaluated
\begin{equation*} \label{newton}
\kappa_{n+1}= \kappa_{n}-\frac{g(a,b,\kappa_{n})-r}{{g}'(a,b,\kappa_{n})}=\kappa_{n}-\frac{g(a,b,\kappa_{n})-r}{\left ( 1-b/\kappa_{n} \right )g\left ( a,b,\kappa_{n} \right )+\left (a/\kappa_{n} \right )-\left (g\left ( a,b,\kappa_{n} \right )  \right )^{2}}.
\end{equation*}
Observe that the iteration can be performed only with one evaluation of the ratio $g(a, b, \kappa)$.

This leads to the final contribution by \cite{watson::writeup1} where the authors have derived iterative bounds to evaluate $g\left ( a,b,\kappa \right )$ for the cases where $0<a<b$. The resulting bounds offer a cheap and efficient way to evaluate the necessary function and show a fast convergence with asymptotically correct behavior. Additionally, it is demonstrated that already the first iteration of the bounds defines the same bounds as in  \cite{watson:Sra+Karp:2013} offering a better approximation for $g\left ( a,b,\kappa \right )$ if more than one iteration is performed. This suggests to combine the techniques from  \cite{watson::writeup1} with the Newton method derived by \cite{watson:Sra+Karp:2013} to solve the estimation problem \ref{kappasol}. 

What is more, since the first iterations are still defined in closed form, one can start the Newton procedure with the mid-value of the bounds and perform a bracketed type of Newton algorithm, which combines derivative-based and bisection steps with the starting brackets given by the bounds \ref{bounds}. With some small adjustments, this is also the to-go and default implementation in the package \pkg{watson}. 

A final possibility is to add the logarithm to the whole procedure and to solve
\begin{equation} \label{kappalogsol}
\log(g\left(a,b, \kappa\right))=\log(r), \quad \text{where} \quad 0<a<b, \text{ and } 0\leq r\leq 1.
\end{equation}
One can show that the Newton step is then defined as
\begin{align} \label{lognewton}
\begin{split}
\kappa_{n+1}&= \kappa_{n}-\frac{\log(g(a,b,\kappa_{n}))-\log(r)}{\log\left ({g}(a,b,\kappa_{n})  \right )'}\\
&=\kappa_{n}-\frac{\log(g(a,b,\kappa_{n}))-\log(r)}{\left ( 1-b/\kappa \right )+\left (a/\left (\kappa_{n} g\left ( a,b,\kappa_{n} \right ) \right ) \right )-g\left ( a,b,\kappa_{n} \right ) },
\end{split}
\end{align}
again requiring only one evaluation of ${g}(a,b,\kappa)$ per iteration. Following the results of \cite{watson::writeup1}, since it holds $$g(a, b, \kappa) = 1 - g(b-a, b, -\kappa), $$ w.l.o.g. one can assume $\kappa<0$, and therefore the above method can numerically help in the cases where the previous derivative is numerically equal to zero, i.e., with extremely small or large values of $\kappa$. 

All of the mentioned cases are implemented in the \pkg{watson} package and the usage will be more discussed in the section \ref{sec4}.

\section{Finite mixtures modeling}\label{sec3}

Finite mixtures modelling is a popular statistical tool in many research areas. These models allow to cluster observations by assuming that for each observation there exists a suitable parametric distribution, which defines a subgroup of the data. The mixture distribution is then a convex combination of the corresponding components, where the weight are usually specified by the affiliation of the data to a given component.

The same way as on a line, also on spheres these models attract a lot of popularity. The EM algorithms for the Watson distribution are for example given in  \cite{watson:Bijral+Breitenbach+Grudic:2007} and  \cite{watson:Sra+Karp:2013}. In the following, for the purpose of clarity, we will stick more to the latter, and hence the matrix notation.
\subsection{ Estimating the parameters of mixtures of Watson distributions}
Suppose we have $x_{1},\dots,x_{n}\in \mathbb{P}^{p-1}$ i.i.d. sample and as before define ${\bf X}$ to be the design matrix with $x_{1},\dots,x_{n}$ as rows. We are interested in the modelling of the data into $K$ multivariate Watson distributions, that together form a mixture distribution. Let $W_{p}(x|\mu_{j},\kappa_{j})$ be the density of the $j$-th component, and $\pi_{j}$ is the corresponding weight. Then the density for a given observation $x_{d}$ is
\begin{equation*}
f(x_{d}|\mu_{1},\kappa_{1}\dots,\mu_{k},\kappa_{k})=\sum_{j=1}^{K}\pi_{j}W_{p}\left ( x_{d}|\mu_{j},\kappa_{j} \right ),
\end{equation*}
with a log-likelihood for the whole data given by
\begin{equation*}
\ell(x_{1},\dots,x_{n}|\mu_{1},\kappa_{1}\dots,\mu_{k},\kappa_{k})= \sum_{i=1}^{n}\log\left(\sum_{j=1}^{K}\pi_{j}W_{p}\left ( x_{i}|\mu_{j},\kappa_{j} \right )\right).
\end{equation*}

The EM algorithm for fitting mixtures of Watson distributions consists of the following steps:

\begin{enumerate}
\item Initialization: Assign the probabilities of the component memberships to each observation. This can  either be done by the user that starts the procedure or randomly. Such initialization allows further preprocessing as for example the diametrical clustering, which will be discussed later in this chapter. Finish the initialization using the M-step which assigns the starting parameters of the mixture distribution using maximum likelihood techniques.

We note that an EM-algorithm is also possible to initialize with the given components parameters and continue with the E-step, however, since  it is considered much easier to have a prior knowledge about the classification rather than the components parameters, this initialization is not offered in \pkg{watson}.

\item Repeat the following procedure until the convergence  or the maximum number of iterations is reached:
\begin{description}
  \item[E-step:] 
  First construct a lower bound for the log-likelihood $\ell(\cdot)$ given by
  \begin{equation*}
   \ell(x_{1},\dots,x_{n}|\mu_{1},\kappa_{1}\dots,\mu_{k},\kappa_{k})\geq \sum_{i,j}\beta_{ij} \log\frac{\pi_{j}W_{p}\left ( x_{i}|\mu_{j},\kappa_{j} \right )}{\beta_{ij} },
   \end{equation*}
   where $\beta_{ij}$ are the posterior defined as
   \begin{equation*}
  \beta_{ij}=\frac{\pi_{j}W_{p}\left ( x_{i}|\mu_{j},\kappa_{j} \right )}{\sum_{k=1}^{K}\pi_{k}W_{p}\left ( x_{i}|\mu_{k},\kappa_{k} \right )}.
   \end{equation*}
   This calculates the probabilities of belonging to a component conditional on the observed values, and defines the so-called soft-E-step. 
   
   From the numerical perspective, it is safer to write this as 
   \begin{equation*}
  \log\beta_{ij}= \log\pi_{j}+\kappa_{j}(x_{i}^t\mu_{j})^2-\log M\left ( \frac{1}{2},\frac{p}{2},\kappa_{k} \right )-\log\left (  \sum_{k=1}^{K}\pi_{k}W_{p}\left ( x_{i}|\mu_{k},\kappa_{k} \right )\right ),
   \end{equation*}
   and hence
   \begin{align*}
   \begin{split}
\log\beta_{ij}=&\log\pi_{j}+\kappa_{j}(x_{i}^t\mu_{j})^2-\log M\left ( \frac{1}{2},\frac{p}{2},\kappa_{k} \right )+m\\&-\log\left (  \sum_{k=1}^{K}\exp\left (\log\pi_{k}+\kappa_{j}(x_{k}^t\mu_{k})^2-\log M\left ( \frac{1}{2},\frac{p}{2},\kappa_{k} \right )-m  \right )\right ),
   \end{split}
   \end{align*}
   where $m = \argmax\limits_{j} \ \log\pi_{j} + \log W_{p}\left ( x_{i}|\mu_{j},\kappa_{j} \right )$.
   
   Further possibility is to use a hard assignment step (see \cite{watson:Sra+Karp:2013}), also called categorical step, where:
   \begin{equation*}
    \beta_{ij}=\left\{\begin{matrix}
1 & \text{if } &j = \argmax\limits_{j'} \ \log\pi_{j'} + \log W_{p}\left ( x_{i}|\mu_{j'},\kappa_{j'} \right ), \\ 
0 & \text{else}.&
\end{matrix}\right.
   \end{equation*}
   If the maximum is not unique, the category is assigned randomly to one of the leading categories. 
   
   A final choice is to use the so-called stochastic step \citep{watson:Celeux}, or S-step, where the category is assigned at random for each observation $i$ to one component $j$, with probability equal to its posterior probability $\beta_{i,j}$.
  \item[M-step:] The M-step is defined by the maximization of the expected log-likelihood by determining separately
for each cluster $j$, the optimal parameters $\mu_{j}$ and $\kappa_{j}$, i.e.,:
   \begin{equation*}
   \mathbf{S}^{j}=\frac{\sum_{i=1}^{n}\beta_{ij}x_{i}x_{i}^{t}}{\sum_{i=1}^{n}\beta_{ij}},\qquad \pi_{j}=\frac{1}{n}\sum_{i=1}^{n}\beta_{ij} ,    \qquad \kappa_{j}=g^{-1}\left ( 0.5,p/2, \mu_{j}^{t}\mathbf{S}^{j}\mu_{j} \right )
   \end{equation*}
   \begin{equation*}
   \mu_{j}=s_{p}^{j} \quad \text{if } \kappa_{j}>0, \qquad \mu_{j}=s_{1}^{j} \quad \text{if } \kappa_{j}<0,
   \end{equation*}
   where $s_{1}^{j},\dots,s_{p}^{j}$ are the eigenvectors of $\mathbf{S}^{j}$ ordered such that the appertaining eigenvalues satisfy $\lambda_{1}^{j}\leq\lambda_{2}^{j}\leq\dots\leq \lambda_{p}^{j}$.
  \item[Convergence check:] Stop if the relative absolute change in the log-likelihood is smaller than a given threshold. Note that the calculation of log-likelihood is strongly connected with the E-step calculations, and thus the convergence is assessed during this procedure. 
     
\end{description}
\end{enumerate}

\subsection{Diametrical clustering}
The directional clustering, in particular the algorithm proposed in \cite{watson:dhillon:2003}, is a well-known clustering technique in bioinformatics. The algorithm uses a non-parametric method and similarly as the EM algorithm, it is based on first picking the category that maximizes the squared scalar product  (E-step), and then defining new concentration directions for each cluster (M-step). 

\cite{watson:Sra+Karp:2013} showed that this algorithm is equivalent to the Watson EM algorithm with the $\kappa_{j}\rightarrow\infty,$ $\forall j=1,\dots,K$, which implies that the $\beta_{i,j}\rightarrow \{0,1\}$. Equivalently, the authors showed that this can be also seen as a hard-assignment EM algorithm where all $\kappa$ parameters are equal and hence can be ignored. This suggests that it is natural to expect a better performance with the Watson mixture modelling (as the diametrical clustering can be seen as a special case), however, thanks to the robustness of the algorithm one may still use the diametrical clustering in the initialization phase as  preprocessing before the main part of the EM starts.

The \pkg{watson} package offers such preprocessing of the model using the \code{init\_iter} parameter in the \fun{watson()} function, which will be more discussed in the next chapter. Alternatively, the user may also directly apply only directional clustering to the data by using the \fun{diam\_clus()} function.

<<echo=FALSE>>=
cat("R> diam_clus(x, k, niter = 100)")
@ 
%
The arguments for this function are as follows.
\begin{description}
\item[\normalfont \code{x}:] A numeric data matrix, with rows corresponding to observations. Can be a dense matrix,
      or any of the supported sparse matrices from \pkg{Matrix} package by \pkg{Rcpp}.
\item[\normalfont \code{k}:] An integer indicating the number of components.
\item[\code{niter}:] Integer indicating the number of
   iterations of the diametrical clustering algorithm, default: 100. 
\end{description}



\subsection{Illustrative example} \label{secHE}

To illustrate the use of the watson distribution to model data on the sphere we use the household
data set from package \pkg{HSAUR2} \citep{watson:HSAUR2}. These data have been already estimated in the directional statistics using the mixture of von Mises-Fisher distributions in \cite{watson::Hornik2014P}. The point of this illustration is to show that if the data are distributed only in the positive orthant, then the watson distribution is capable of doing the same work as a more specified von Mises-Fisher distribution.


\begin{figure}[h]
\centering
      \addheight{\includegraphics[width=43mm]{true.png}} %
    \hspace{1cm}  \addheight{\includegraphics[width=43mm]{K2.png}} \\
      \addheight{\includegraphics[width=43mm]{K3.png}} %
    \hspace{1cm} \addheight{\includegraphics[width=43mm]{mK.png}} \\
       \caption{Household expenses data with gender indicated by color after projection to the
sphere together with the estimated $\mu$ parameters at the top left, estimated mixtures of watson distributions with K = 2
and K = 3 at the top right and bottom left, respectively and results of an estimation if minweight is set to 0.15 at the bottom right.}
\end{figure}

The data consist of the expenditures on four commodity groups of 20 single men
and 20 single women. In the following, similarly as in \cite{watson::Hornik2014P}, we will focus only on the expenditure on housing, foodstuffs and services, in order to be able to visualize the data and the results.  The data points are projected onto the sphere by normalizing them, and thus, in the following analysis we are interested in finding groups of households
which has similar proportion of expanses rather than households that differ in their total absolute expenses.

During the fitting procedure, the gender information is not used and it is investigated if the finite mixtures can recognize the groups just by the angular similarities between the expanses.

If the model is estimated with the assumption of two underlying unobserved groups, the results deliver only one misclassification. This is the same result as in \cite{watson::Hornik2014P}. The same holds for the situation where $K=3$, which was also with the mixture of Watson distributions observed to be the model with the lowest BIC (see for example \cite{MCLA2000}), with only $K = 1,\dots,5$ considered. Finally, if the model is estimated under the condition that the every group must contain at least $13\%$ of the data and the estimation starts with $K=6$, the best model coincide with the one estimated using $K=2$. The estimated parameters and the BIC value are given in Table \ref{tab}. The R code for reproducing these results is provided in Section \ref{sec42} after introducing package \pkg{watson}.




\begin{table}[]
\centering
\begin{tabular}{cccccccc}
\hline
 & $\pi$ & housing & food & service & $\kappa$ & BIC  \\ 
\hline
\multirow{ 2}{*}{$K=2$} & 0.53 & 0.66 & 0.64 & 0.40 & 10.21 & \multirow{2}{*}{-144.49}  \\ 
 & 0.47 & 0.95 & 0.13 & 0.27 & 57.44 &   \\ 
\hline
\multirow{ 3}{*}{$K=3$} & 0.13 & 0.67 & 0.31 & 0.68 & 91.58 & \multirow{3}{*}{-156.04}   \\ 
 & 0.35 & 0.59 & 0.76 & 0.28 & 32.34 &   \\ 
 & 0.52 & 0.95 & 0.15 & 0.27 & 42.70 &  \\ 
\end{tabular}
 \caption{Estimated results of finite mixtures of Watson distributions to the household data. } \label{tab}
\end{table}

<<echo = FALSE, eval=FALSE>>=
library(watson)
data("household", package = "HSAUR2")
x <- household[, c(1, 2, 4)]
gender <- household$gender

mu <- rbind(t(watson(x[gender == "female",], k = 1)$mu), 
            t(watson(x[gender == "male",], k = 1)$mu))
rownames(mu) <- c("female", "male")

   
set.seed(10)
wat <- lapply(1:5, function(K) watson(x, k = K))
bic = sapply(wat, BIC)

watt <-  watson(x, k = 6, minweight = 0.13, nruns = 100)

library(rgl)
x <- x/sqrt(rowSums(x^2))
rgl.open()
view3d( theta = 62, phi = 25, zoom =0.6)
rgl.bg(color = "white")
rgl.points(x[,1],x[,2],x[,3], ylim=c(-1,1), col = c(gender), xlim=c(-1,1), zlim = c(-1,1), xlab = "x", ylab = "y", zlab = "z", alpha = 0.8)
rgl.points(mu[,1],mu[,2],mu[,3], ylim=c(-1,1), col = c(1,2), xlim=c(-1,1), zlim = c(-1,1), xlab = "x", ylab = "y", zlab = "z", alpha = 0.5, size = 10)
spheres3d(x = 0, y = 0, z = 0, radius = 0.98, col = "green", alpha = 0.6, back = "lines")
rgl.lines(c(-0.98,0.98), c(0, 0), c(0, 0), color = "black")
rgl.lines(c(0, 0), c(-0.98,0.98), c(0, 0), color = "blue")
rgl.lines(c(0, 0), c(0, 0), c(-0.98,0.98), color = "red")
bgplot3d({
  plot.new()
  title(main = 'Known group membership  ', line = 3.2)
  # use here any other way you fancy to write your title
})
rgl.snapshot("true.png")

w2 <- wat[[2]]
mu2 <- t(w2$mu_matrix)
rgl.open()
view3d( theta = 62, phi = 25, zoom =0.6)
rgl.bg(color = "white")
rgl.points(x[,1],x[,2],x[,3], ylim=c(-1,1), col = -predict(w2)+3, xlim=c(-1,1), zlim = c(-1,1), xlab = "x", ylab = "y", zlab = "z", alpha = 0.8)
rgl.points(mu2[,1],mu2[,2],mu2[,3], ylim=c(-1,1), col = c(2,1), xlim=c(-1,1), zlim = c(-1,1), xlab = "x", ylab = "y", zlab = "z", alpha = 0.5, size = 10)
spheres3d(x = 0, y = 0, z = 0, radius = 0.98, col = "green", alpha = 0.6, back = "lines")
rgl.lines(c(-0.98,0.98), c(0, 0), c(0, 0), color = "black")
rgl.lines(c(0, 0), c(-0.98,0.98), c(0, 0), color = "blue")
rgl.lines(c(0, 0), c(0, 0), c(-0.98,0.98), color = "red")

bgplot3d({
  plot.new()
  title(main = 'Mixture of Watsons with K = 2', line = 3.2)
  # use here any other way you fancy to write your title
})
rgl.snapshot("K2.png")

w3 <- wat[[3]]
mu3 <- t(w3$mu_matrix)
rgl.open()
view3d( theta = 62, phi = 25, zoom =0.6)
rgl.bg(color = "white")
rgl.points(x[,1],x[,2],x[,3], ylim=c(-1,1), col = ifelse(-predict(w3)+4==3, 4, -predict(w3)+4) , xlim=c(-1,1), zlim = c(-1,1), xlab = "x", ylab = "y", zlab = "z", alpha = 0.8)
rgl.points(mu3[,1],mu3[,2],mu3[,3], ylim=c(-1,1), col = c(4,2,1), xlim=c(-1,1), zlim = c(-1,1), xlab = "x", ylab = "y", zlab = "z", alpha = 0.5, size = 10)
spheres3d(x = 0, y = 0, z = 0, radius = 0.98, col = "green", alpha = 0.6, back = "lines")
rgl.lines(c(-0.98,0.98), c(0, 0), c(0, 0), color = "black")
rgl.lines(c(0, 0), c(-0.98,0.98), c(0, 0), color = "blue")
rgl.lines(c(0, 0), c(0, 0), c(-0.98,0.98), color = "red")

bgplot3d({
  plot.new()
  title(main = 'Mixture of Watsons with K = 3', line = 3.2)
  # use here any other way you fancy to write your title
})
rgl.snapshot("K3.png")

w4 <- watt
mu4 <- t(w4$mu_matrix)
rgl.open()
view3d( theta = 62, phi = 25, zoom =0.6)
rgl.bg(color = "white")
rgl.points(x[,1],x[,2],x[,3], ylim=c(-1,1), col = predict(w4), xlim=c(-1,1), zlim = c(-1,1.2), xlab = "x", ylab = "y", zlab = "z", alpha = 0.8)
rgl.points(mu4[,1],mu4[,2],mu4[,3], ylim=c(-1,1), col = c(1,2), xlim=c(-1,1), zlim = c(-1,1), xlab = "x", ylab = "y", zlab = "z", alpha = 0.5, size = 10)
spheres3d(x = 0, y = 0, z = 0, radius = 0.98, col = "green", alpha = 0.6, back = "lines")
rgl.lines(c(-0.98,0.98), c(0, 0), c(0, 0), color = "black")
rgl.lines(c(0, 0), c(-0.98,0.98), c(0, 0), color = "blue")
rgl.lines(c(0, 0), c(1,1.2), c(0, 0), color = "white", alpha = 0)
rgl.lines(c(0, 0), c(0, 0), c(-0.98,0.98), color = "red")

bgplot3d({
  plot.new()
  title(main = 'Mixture of Watsons with \n minweight = 0.15', line = 2)
  # use here any other way you fancy to write your title
})
rgl.snapshot("mK.png")
tab2 <- table(predict(wat[[2]]), household$gender)
tab3 <- table(predict(wat[[3]]), household$gender)
@







\section{Software}\label{sec4}
\subsection{\fun{watson()}}
The main function of the \pkg{watson} package is the \fun{watson()} function for fitting mixtures of Watson
distributions. It can be called as:
%
<<echo=FALSE>>=
cat("R> watson(x, k, control = list(), ...)")
@ 
%
The arguments for this function are as follows.
\begin{description}
\item[\normalfont \code{x}:] A numeric data matrix, with rows corresponding to observations. Can be a dense matrix,
      or any of the supported sparse matrices from \pkg{Matrix} package by \pkg{Rcpp}.
\item[\normalfont \code{k}:] An integer indicating the number of components.
\item[\normalfont \code{control}:] A list of control parameters consisting of
 \begin{description}
  \item[\code{M}:] This argument allows to specify how to estimate
   the concentration parameters during the M-step:
   The method for solving for the concentration parameters can
     be \mbox{specified} by one of \code{"Sra\_2007"},
     \code{"BBG"}, \code{"BBG\_c"},
     \code{"Sra\_Karp\_2013"}, \code{"bisection"}, \code{"lognewton"} and
     \code{"newton"} (default). For more details on these methods see
     Section~\ref{sec2}.
   \item[\code{E}:] Specifies the transformation of posterior probabilities during the E-step. Possible values \code{"softmax"} (default), \code{"hardmax"}, and \code{"stochmax"}. For more details on these methods see
     Section~\ref{sec3}.
 \item[\code{converge}:] Logical indicating if convergence of the
   algorithm should be checked and stopped if the relative change in likelihood is smaller than \code{reltol}. 
   In case \code{converge = FALSE}, all iterations are performed and the one with maximum achieved likelihood
   is returned. This makes mostly sense and is also the default case if  \code{E} is equal to \code{"softmax"}.  
  \item[\code{maxiter}:] Integer indicating the maximum number of
   iterations of the EM algorithm, default: 100.
  \item[\code{reltol}:] Relative change threshold. Ignored if \code{converge = FALSE}, default: \verb|sqrt(.Machine$double.eps)|.
  \item[\code{ids}:] Indicates either the class memberships of the
    observations or if equal to \code{TRUE} the class memberships are
    obtained from the attributes of the data. In this way the class
    memberships are for example stored if data is generated using the
    function \code{rmwat()}. If this argument is specified, the EM
    algorithm is stopped after initialization, i.e., the parameter
    estimates are determined conditional on the known true class
    memberships (one M-step is performed).
   \item[\code{init\_iter}:] 
   a numeric vector setting the number of diametrical clustering iterations to do, before the EM starts, default: 0.
  \item[\code{start}:]  A specification of the starting values to be employed. Can be a list of matrices giving the memberships
   of objects to components. This information is combined with the \code{init\_iter} parameter and together form
   the initialization procedure. If not given, the starting values are drawn randomly.
   
   If several starting values are specified, the EM algorithm is performed individually to each starting 
   value and the best solution found is returned.
  \item[\code{nruns}:] Integer giving the number of EM runs to be performed. Default: 1. Only used if \code{start} or \code{ids} is not given.
  \item[\code{minweight}:]  Numeric indicating the minimum prior probability. Components falling below this threshold are removed
   during the iteration. If $\geq 1$, this is taken as the minimal number of observations in a component, default: 0 if
  \code{E = "softmax"} and 2 if other type of E-method is used.
  \item[\code{N}:]  Integer indicating number of iterations used when the Kummer's function and its ratio are approximated, default: 30.
  \item[\code{verbose}:] a logical indicating whether to provide some output on algorithmic progress, default: getOption("verbose").
  \end{description}
\end{description}


The object returned by \fun{watson()} is of the \class{watfit} class with available methods \fun{print()}, \fun{coef()}, \fun{logLik()} and
\fun{predict()} (yields either the component assignments or the matrix of a-posteriori probabilities). Finally, the functions \fun{rmwat()} and \fun{diamclus()} for simulation of mixtures of Watson distributions and diametrical clustering are available. 

\subsection{Illustrative example: Hausehold expanses} \label{sec42}

This chapter contains the code for reproducing the results of the presented example in the Section \ref{secHE}. First the data are loaded and the columns housing, foodstuff and service are extracted and stored in the variable \code{x}. Additionally, the gender classification is extracted and used to estimate the mean direction for both genders separately. 

The function \fun{watson()} is then used to estimate the mixture of Watson distributions with the number of components varying from 1 to 5 and the BIC values are reported.  The last estimation sets the \code{minweight} parameter to 0.13, in order to avoid clusters where only insignificant number of elements is grouped together. For this estimation, the fitting is repeated 100 timed, to avoid sub-optimas where EM algorithm could be trapped in some local optimum. This model is in addition printed out.

<<>>=
library(watson)
data("household", package = "HSAUR2")
x <- household[, c(1, 2, 4)]
gender <- household$gender

mu <- rbind(t(watson(x[gender == "female",], k = 1)$mu), 
            t(watson(x[gender == "male",], k = 1)$mu))

   
set.seed(10)
wat <- lapply(1:5, function(K) watson(x, k = K))
sapply(wat, BIC)

(watt <-  watson(x, k = 6, minweight = 0.13, nruns = 100))
@



\section{Numerical issues} \label{sec5}

In the following, we will use the notation from \ref{sec2} and write 
\begin{equation*}
g(a, b, z) = \frac{\mathrm{d} \log(M(a, b, z)))}{\mathrm{d} z} =  \frac{{M(a, b, z)}'}{M(a, b, z)} = \frac{a}{b}\frac{{M(a+1, b+1, z)}}{M(a, b, z)}.
\end{equation*}
As shown in Section \ref{sec2} and \ref{sec3}, computing ML estimation of concentration parameters for Watson distributions on $\mathbb{R}^p$ necessitates the solution $\kappa$ of
$$g\left(a,b, \kappa\right)=r, \quad \text{where} \quad 0<a<b, \text{ and } 0\leq r\leq 1.$$
Furthermore, a computation of log-likelihoods or the a-posteriori probabilities in the mixture modeling requires to evaluate the expressions as 
$\log(M(a, b, z))$, where $0<a<b$. Because $M(a, b, z)\rightarrow\infty$ as $z\rightarrow\infty$, a direct evaluation of $M(\cdot)$ with further logarithm or quotient function application is clearly not a good idea, mostly if we know that the fraction satisfies $0<g(a, b, z)<1$. In general, it can be observed that the Kummer's function easily over- or underflows for very general set of parameters just because of the geometric series structure the function embraces. 

\subsection{Approximation of the Kummer's ratio}

We assume $b>a>0,$ $z<0$ and follow the approach used in \cite{watson::writeup1}. Define the sequences of real numbers through the recursive relation as 
\begin{equation*} 
l_{n}^{(0)}(z)=\frac{2\left (a +n \right )}{\sqrt{(z-\left ( b +n \right ))^2 + 4 \left (a+n  \right ) z} -z + \left (b +n \right )},
\end{equation*}
\begin{equation*} 
l_{n}^{(m)}(z)=\frac{a+n}{b+n-z+zl_{n+1}^{(m-1)}(z)},
\end{equation*}
and 
\begin{equation*} 
u_{n}^{(0)}(z)= 1 - \frac{2\left ( b -a  \right )}{\sqrt{\left ( z+\left (b +n \right )+1 \right )^{2}-4(b -a+1 )z}+z+\left (b+n  \right )-1},
\end{equation*}
\begin{equation*} 
u_{n}^{(m)}(z)=\frac{a+n}{b+n-z+zu_{n+1}^{(m-1)}(z)}.
\end{equation*}
The sequences $\left(l_{0}^{(0)}(z),l_{0}^{(1)}(z),l_{0}^{(2)}(z),\dots \right)$ and  $\left(u_{0}^{(0)}(z),u_{0}^{(1)}(z),u_{0}^{(2)}(z),\dots \right)$  converge monotonically to $g(a,b,z)$  from below and above, respectively. For the proof see \cite{watson::writeup1}. 

Using another result of \cite{watson::writeup1}, i.e., $g(a, b, z) = 1 - g(b-a, b, -z),$ the evaluation routine can be easily described by the algorithm \ref{A1}, where the approximation function calculates $l_{0}^{(N)}(z)$ and $u_{0}^{(N)}(z)$ and returns the middle value in between.
\begin{algorithm}
\label{A1}
\caption{Kummer's function algorithm}
\begin{algorithmic}[1]
\Procedure{Kummer's}{$a,b,z,N$ = number of iterations}
\If {$z =0$} \Return $a/b$
\EndIf
\If {$z <0$} g = approximate($a,b,z,N$)
\State \Return $g$
\EndIf
\If {$z>0$} g = approximate($b-a,b,-z,N$)
\State \Return $1-g$
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

The convergence  of the irrational bounds for the values $a = 0.5,$ $b=50$ and $a=99.5,$ $b=100$ is visualized in the following figures. Note that the parameters were specifically chosen to be either close or far away from each other as this makes the approximation more difficult, because the crossing point $z=0$, where the bounds are exact, is either after or before the steepest part of the function, respectively. The ``true" values were calculated using the Mathematica software for higher precision.

\noindent
<<echo=FALSE,message=FALSE,fig.height=6.5,fig.width=10, cache=TRUE>>=
library(ggplot2)
library(mistr)
iratio_gneg <- function(a,b,z,N,change=FALSE){
   start1 <- (2*(a+N))/(b+N-z+sqrt((z-b-N)^2+4*(a+N)*z)) 
   start2 <- 1- (2*(b-a))/((b+N)-1 + z + sqrt(-4*((b-a)+1)*z + (z + (b+N+1))^2 ) )
   s <- c(start1,start2)
   if(N>0){
      for(i in (N-1):0){
      s <- (a+i)/((b+i) - z + z * s)
      }
   }
   if(change) s=1-s
   data.frame(y=s,type= as.factor(rep(c("l","u"),each =length(z))))
}
g <- function(a,b,z,N){
   m <- data.frame(y = numeric(length(z)*2), type = factor(c("l","u")))
   z2 <- rep(z,2)
   m[z2==0,] <- data.frame(y=rep(a/b, sum(z2==0)),type = factor(rep(c("l","u"),each = sum(z==0))))
   m[z2<0,] <- iratio_gneg(a,b,z[z<0],N)
   m[z2>0,] <- iratio_gneg(b-a,b,-z[z>0],N,change = TRUE)
   m
}
x = seq(-100,200,0.1)
a=0.5
b=50
N=3
d<- do.call("rbind", lapply(c(0,1,5), function(i) data.frame(x =x, g(a,b,x,i))))
d <- cbind(d, N= as.factor(rep(c(0,1,5), each = 2*length(x))))
a05b50 = read.csv(file = "~/Dropbox/Watson/a05b50",header = FALSE)
colnames(a05b50) <- "y"
d2 <- cbind(x,a05b50, type = as.factor("l"), N = as.factor("true"))
d3 <- rbind(d,d2)
# d <- rbind(d,cbind(data.frame(x =x, g(a,b,x,30)), N= as.factor(rep(30, each = 2*length(x)))))
p1 =ggplot(d3)+
   # geom_line(data = d2, aes(x=x, y=V1), size = 1.8, alpha=0.9, color = "#f28df9")+
   geom_line(aes(x,y,col=N, lty = type), size = 0.8) +
   mistr_theme(legend.position = "top")+
   ggplot2::labs(x = NULL, y = NULL, title = expression("g"~(alpha~"=0.5,"~beta~"=50")))

d<- do.call("rbind", lapply(c(0,1,5,10,30), function(i) data.frame(x =x, g(a,b,x,i))))
d <- cbind(d, N= as.factor(rep(c(0,1,5,10,30), each = 2*length(x))))
d$y <- d$y-a05b50$y
p2 =ggplot(d)+
   # geom_line(data = d2, aes(x=x, y=V1), size = 1.8, alpha=0.9, color = "#f28df9")+
   geom_line(aes(x,y,col=N, lty = type), size = 0.8) +
   mistr_theme(legend.position = "top")+
   ggplot2::labs(x = NULL, y = NULL, title = expression("g"~(alpha~"=0.5,"~beta~"=50")~"-differences"))


   
x = seq(-500,100,0.1)
a=99.5
b=100
N=50
d<- do.call("rbind", lapply(c(0,1,5), function(i) data.frame(x =x, g(a,b,x,i))))
d <- cbind(d, N= as.factor(rep(c(0,1,5), each = 2*length(x))))
a995b100 = read.csv(file = "~/Dropbox/Watson/a995b100",header = FALSE)
colnames(a995b100) <- "y"
d2 <- cbind(x,a995b100, type = as.factor("l"), N = as.factor("true"))
d3 <- rbind(d,d2)
p3=ggplot(d3)+
   # geom_line(data = d2, aes(x=x, y=V1), size = 1.8, alpha=0.9, color = "#f28df9")+
   geom_line(aes(x,y,col=N, lty = type), size = 0.8) +
   mistr_theme(legend.position = "none")+
   ggplot2::labs(x = NULL, y = NULL, title = expression("g"~(alpha~"=99.5,"~beta~"=100")))

d<- do.call("rbind", lapply(c(0,1,5,10,30), function(i) data.frame(x =x, g(a,b,x,i))))
d <- cbind(d, N= as.factor(rep(c(0,1,5,10,30), each = 2*length(x))))
d$y <- d$y-a995b100$y
p4=ggplot(d)+
   # geom_line(data = d2, aes(x=x, y=V1), size = 1.8, alpha=0.9, color = "#f28df9")+
   geom_line(aes(x,y,col=N, lty = type), size = 0.8) +
   mistr_theme(legend.position = "none")+
   ggplot2::labs(x = NULL, y = NULL, title=expression("g"~(alpha~"=99.5,"~beta~"=100")~"-differences"))
mistr:::multiplot(p1,p3,p2,p4,cols=2)
@
The plot with the differences indicates that even with the chosen parameters, after 5 iteration the irrational bounds are decently close and after 30 iterations almost exact. Furthermore, it should be pointed out that the iterations are composed only by four very simple arithmetic operations, thus in a compiled programming languages as for example C or C++ such estimation requires almost no time even for big $N$.

To solve the inverse of $g(a, b, z)$ one combines this evaluation with a bracketed type of Newton algorithm from section \ref{sec2}. Note that $$u_{a,b}^{(0)^{-1}}(r)=L(r), \quad u_{a,b}^{(1)^{-1}}(r)=K(r), \quad l_{a,b}^{(1)^{-1}}(r)=U(r)$$ (see Theorem~6 of \cite{watson::writeup1}) offers exactly such brackets for the starting value. 

Even better, the authors in \cite{watson::writeup3} showed that for $r>\frac{w^2a+w(b-a)a}{(b+1)(b-a)+w^2a}$, where $w_{a,b} = \frac{\sqrt{16ab+8a+1}+4a+1}{8a}$ the bound 
$$
B^{-1}_{c_{3},c_{4}}(a,b,r) = \frac{rb-a }{r\left ( 1-r \right )}\frac{a\left ( c_3+c_4 \right )+br\left ( c_3-c_4 \right )   }{2ab },
$$
with $c_3 = b+w_{a,b}$ and $c_4=\frac{ c_3\left ( a+b \right )-2ab}{b-a}$ is a better upper bound than $u_{a,b}^{(1)^{-1}}(r)$. Since the iteration is done on the negative reals, i.e., $r < \frac{a}{b}$, this motivates the following result.

\begin{prop}
Let $b>a>0$ and define $$\gamma_{a,b}^{-1}(r) =\frac{rb-a }{r\left ( 1-r \right )}\frac{a\left ( c_3+c_4 \right )+br\left ( c_3-c_4 \right )   }{2ab }$$ with $c_3 = b+w_{a,b}$,  $w_{a,b} = \frac{\sqrt{16ab+8a+1}+4a+1}{8a}$ and $c_4=\frac{ c_3\left ( a+b \right )-2ab}{b-a}$. Then 
$$
g^{-1}(a,b,r)>\delta_{a,b}^{-1}(r) = -\gamma_{b-a,b}^{-1}(1-r) > u_{a,b}^{(1)^{-1}}(r) \quad \text{for} \quad r< \frac{(b+1)a-w_{b-a,b}(b-a)a}{(b+1)a+w_{b-a,b}^2(b-a)}.
$$
\end{prop}
\begin{proof}
Let $r'>\frac{w_{a',b}^2a'+w_{a',b}(b-a')a'}{(b+1)(b-a')+w_{a',b}^2a'}$ then it holds that (see \cite{watson::writeup3}) 
$$
g^{-1}(a',b,r') < \gamma_{a',b}^{-1}(r') < u_{a',b}^{(1)^{-1}}(r').
$$
Thus let $a = b - a'$ and $r = 1- r'$, from which 
$$
-g^{-1}(b-a,b,1-r) > \delta_{a,b}^{-1}(r) = -\gamma_{b-a,b}^{-1}(1-r) > -u_{b-a,b}^{(1)^{-1}}(1-r ) 
$$
$$
\text{for} \quad r< 1 - \frac{w_{b-a,b}^2(b-a)+w_{b-a,b}(b-a)a}{(b+1)a+w_{b-a,b}^2(b-a)}= \frac{(b+1)a-w_{b-a,b}(b-a)a}{(b+1)a+w_{b-a,b}^2(b-a)}.
$$
Additionally, from the form of 
$$
u_{a,b}^{(1)^{-1}}(r) = \frac{br-a}{2r(1-r)}\left ( 1+\sqrt{1+\frac{4\left ( b+1 \right )r\left ( 1-r \right )}{a\left ( b-a \right )}} \right ),
$$
it is easy to observe that $u_{a,b}^{(1)^{-1}}(r) = -u_{b-a,b}^{(1)^{-1}}(1-r)$, which is true also for $g(\cdot)$, as it holds that 
$g(a,b,z) = 1-g(b-a,b,-z)$ (see \cite{watson::writeup1}) and thus $g^{-1}(a,b,r) = -g^{-1}(b-a,b,1-r)$. This completes the proof.
\end{proof}

Note that $\delta_{a,b}^{-1}(r)$ is an upper bound on the whole negative reals, i.e., $r < \frac{a}{b}$, however only for sufficiently small $r$ dominates $u_{a,b}^{(1)^{-1}}(r)$. 

Thus if  $r$ is sufficiently small or big, the bounds $\gamma_{a,b}^{-1}(r)$ and $\delta_{a,b}^{-1}(r)$  provide better bounds for the starting bracket. Exactly this idea is implemented in the \fun{watson()} and is employed if \code{M} is set either to \code{"bisection"}, \code{"lognewton"} or \code{"newton"}.    

\subsection{Approximation of the Kummer's function logarithm}

Since $g(a,b,z)$ is essentially the logarithmic derivative of the Kummer's function $M(a,b,z)$, it can be also directly used to calculate its logarithm. For the following let $m(a,b,z) = \log(M(a,b,z))$ and $0<a<b$. Since $M(a,b,0)=1$ $\forall \ a,b$, the general formula can be written as
\begin{equation*}
m(a,b,z) = m(a,b,0) + \int_{0}^{z}g(a,b,z) dz= \int_{0}^{z}g(a,b,z) dz, 
\end{equation*}
which simplifies the problem o the integration of $g(a,b,z)$ on a compact set. 

We note that, while this allows for simpler solutions, as for example the numerical integration of $g(\cdot)$, \pkg{watson} employes more controlled approaches, which will be discuses in the following sections. 

\subsubsection{Bounds for Kummer's function logarithm}

This method uses the bounds derived in \cite{watson::writeup3}. Defining a family of bounds 
$$
B_{c_{3},c_{4}}(a,b,z) =\frac{\frac{a}{b}(c_{3}+c_{4})}{\sqrt{z^2+2\left ( \frac{a}{b}(c_{3}+c_{4}) - c_{4} \right )z+c_{3}^2}-z+c_{4}} , \quad \text{ with } c_{3}> 0, -c_{3}<  c_{4}\leq  c_{3}\frac{a+b}{b-a}
$$
and the bounds 
\begin{equation*}
  S_{c_3,c_4}^{+}(a,b,l)=\int_{0}^{l} B_{c_3,c_4}(a,b,z)\ {\mathrm{d} z} = lB_{c_3,c_4}(a,b,l)- \left(I_{c_3,c_4}\left(a,b,B_{c_3,c_4}(a,b,l)\right) - I_{c_3,c_4}\left(a,b,\frac{a}{b}\right)\right) ,
  \end{equation*}
  for $l\geq 0$ and 
  \begin{align*}
  S_{c_3,c_4}^{-}(a,b,l) &= -\int_{0}^{-l}1- B_{c_3,c_4}(a,b,z)\ {\mathrm{d} z} = \\
  &=l(1-B_{c_3,c_4}(a,b,-l)) - \left(I_{c_3,c_4}\left(a,b,B_{c_3,c_4}(a,b,-l)\right) - I_{c_3,c_4}\left(a,b,\frac{a}{b}\right)\right),
  \end{align*}
for $l\leq 0$, where
 $$I_{c_3,c_4}(a,b,r) = \frac{-a^2 (c_3+c_4) \log (r)-(b-a) \log (1-r) (a (c_3+c_4)+b (c_3-c_4))+b^2 r (c_4+c_3)}{2 a b},$$
the authors have shown that 
\begin{align*}
  \forall c_{3_{(1)}},c_{4_{(1)}},c_{3_{(2)}},c_{4_{(2)}}: B_{c_{3_{(1)}},c_{4_{(1)}}}(a,b,z) \leq &g(a,b,z) \leq B_{c_{3_{(2)}},c_{4_{(2)}}}(a,b,z) \quad \forall z\geq 0 \\ 
  \Rightarrow S_{c_{3_{(1)}},c_{4_{(1)}}}^{+}(a,b,l) \leq &m(a,b,l) \leq S_{c_{3_{(2)}},c_{4_{(2)}}}^{+}(a,b,l)\quad \forall l \geq 0,
\end{align*}
  \begin{align*}
  \forall c_{3_{(1)}},c_{4_{(1)}},c_{3_{(2)}},c_{4_{(2)}}: B_{c_{3_{(1)}},c_{4_{(1)}}}(a,b,z) \leq& g(a,b,z) \leq B_{c_{3_{(2)}},c_{4_{(2)}}}(a,b,z)\quad \forall z\geq 0 \\
  \Rightarrow S_{c_{3_{(1)}},c_{4_{(1)}}}^{-}(b-a,b,l) \leq& m(a,b,l) \leq S_{c_{3_{(2)}},c_{4_{(2)}}}^{-}(b-a,b,l) \quad\forall l \leq 0.
\end{align*}
For more details see  \cite{watson::writeup3}.

This method defines easily accessible bounds for $m(a,b,z)$. What is more, the authors showed that if for both (upper and lower) bounds the subfamily with $c_4 = (c_3(a+b)-2ab)/(b-a)$ is used, the error is at most 
$
\left (c_{3_{(1)}} -c_{3_{(2)}}   \right )\left ( 1+\frac{a\log(\frac{a}{b})}{b-a} \right ).
$
Note that if $z\geq 0$, the best known upper and lower bounds are $c_3 = b+1$ and $c_3 = b+w_{a,b}$ with $w_{a,b} = \frac{\sqrt{16ab+8a+1}+4a+1}{8a}$ and $c_4=\frac{ c_3\left ( a+b \right )-2ab}{b-a}$ ,respectively, where the first bound is equivalent to $u_{a,b}^{(0)}(z)$.

While this subfamily has finite error, it is dominated around 0 by the subfamily $c_3 = b+1$. This motivates to combine this bounds as
$$
V_{max}(a,b,z) = \max\left(B_{b+1 ,\frac{(b+1)(b^2-a(b-2))}{(b-a)(b+2)} }(a,b,z), B_{\frac{8 a b+\sqrt{16 a b+8 a+1}+4 a+1}{8 a} ,(c_3(a+b)-2ab)/(b-a) }(a,b,z)\right)
$$
with the inverse 
$$
V^{-1}_{max}(a,b,r) = 
\left\{\begin{matrix}
B^{-1}_{b+1 ,\frac{(b+1)(b^2-a(b-2))}{(b-a)(b+2)} }(a,b,r) & \text{for} & r\leq\frac{a\left (  w_{a,b}\left ( b+2 \right )-1-a  \right )}{w_{a,b}a\left ( 2+b \right )+\left ( 1+b \right )\left ( b-2a \right )}\\ 
B^{-1}_{b+w_{a,b} ,(c_3(a+b)-2ab)/(b-a) }(a,b,r) & \text{for} & r>\frac{a\left (  w_{a,b}\left ( b+2 \right )-1-a  \right )}{w_{a,b}a\left ( 2+b \right )+\left ( 1+b \right )\left ( b-2a \right )}\
\end{matrix}\right.,
$$
where as before $w_{a,b} =\frac{\sqrt{16 a b+8 a+1}+4 a+1}{8 a} $ allows to define a superior bound for the integral as
$$
R_{max}(a,b,l) = \int_{0}^{l} V_{max}(a,b,z) )\ {\mathrm{d} z}
$$
which can be again decomposed and evaluated using the inverse. Clearly, for $l\geq0$ as before
\begin{align*}
\begin{split}
R_{max}(a,b,l) =  \int_{0}^{l} V_{max}(a,b,z)\ {\mathrm{d} z} = lV_{max}(a,b,z)- \int_{0}^{V_{max}(a,b,l)} V^{-1}_{max}(a,b,r) \ {\mathrm{d} r}\\
=lV_{max}(a,b,z)- \int_{0}^{\min\left (\gamma , V_{max}(a,b,l)   \right )} B^{-1}_{b+1 ,\frac{(b+1)(b^2-a(b-2))}{(b-a)(b+2)} }(a,b,r) \ {\mathrm{d} r} \\ - \int_{\gamma}^{\max\left (\gamma , V_{max}(a,b,l)   \right )} B^{-1}_{\frac{8 a b+\sqrt{16 a b+8 a+1}+4 a+1}{8 a} ,(c_3(a+b)-2ab)/(b-a) }(a,b,r) \ {\mathrm{d} r},  
\end{split}
\end{align*}
with $w_{a,b} =\frac{\sqrt{16 a b+8 a+1}+4 a+1}{8 a} $ and $\gamma = \frac{a\left (  w_{a,b}\left ( b+2 \right )-1-a  \right )}{w_{a,b}a\left ( 2+b \right )+\left ( 1+b \right )\left ( b-2a \right )}$. Note that while this form seems complicated, for a computer is this a trivial task as the integrals attain a closed form. This additionally improves the maximal error by the difference of the bounds up to the crossing value $\gamma$, i.e.,
$$
\int_{0}^{\gamma} B^{-1}_{b+1 ,\frac{(b+1)(b^2-a(b-2))}{(b-a)(b+2)} }(a,b,r) -  B^{-1}_{\frac{8 a b+\sqrt{16 a b+8 a+1}+4 a+1}{8 a} ,(c_3(a+b)-2ab)/(b-a) }(a,b,r) \ {\mathrm{d} r}.
$$

The package \pkg{watson} uses the above defined techniques, more specifically the mean of bounds based on $u_{a,b}^{(0)}(z)$ for the upper bound and $V_{max}(a,b,z)$ for the lower bound to approximate $m(a,b,z)$, while controlling for the error. If the difference between the bounds is relatively ``too big", the package will continue with the following ad-hoc method. 



\subsubsection{Ad-hoc method for the Kummer's function logarithm}

Another approach is to use the properties of $g(a,b,z)$ and rewrite $m(a,b,z)$ as a sum of parts which form it. This would still require one evaluation of the Kummer's function, but one can choose the parameters for which $M(a,b,z)$ either does not underflow or overflow. An example of such a case is, e.g., $M(a,b,z)$ where $a\leq 1$, $b<2$ and $z$ is negative. For such a set of parameters the evaluation using gsl library does not underflow even for cases as $z=-1\times 10^{300}$ and additionally the approximation error is negligible. This behavior is presented because the rising factorials of $M(a,b,z)$ do not increase fast enough. For the following recall that $a=0.5$, the derivation for a more general case can be found in  \cite{watson::writeup1}. Formally,
\begin{equation*}
m(a,b,z)= z+m(b-a,b,-z),
\end{equation*}
where we applied the Kummer's identity $M(a,b,z)=e^{z}M(b-a,b,-z)$. Thus, 
\begin{align*}
\begin{split}
m(a,b,z)=&z +\log(g(b-a-1,b-1,-z))+\log(b-1)-\log(b-a-1)\\
+&m(b-a-1,b-1,-z). 
\end{split}
\end{align*}
Recursively rewriting the above expression yields
\begin{align*}
\begin{split}
=z&+\underbrace{\sum_{i=1}^{\lfloor b-a\rfloor_{s}} \left ( \log(g(b-a-i,b-i,-z))+\log(b-i)-\log(b-a-i) \right )}_{S}\\
&+m(b-a-\lfloor b-a\rfloor_{s},b-\lfloor b-a\rfloor_{s},-z)
\end{split}
\end{align*}
where $\lfloor x\rfloor_{s}=\max_{m\in\mathbb{Z}}m<x$ is the strict floor operator. Hence
\begin{equation*}
m(a,b,z)=\left\{\begin{matrix}
S+z+m(b-a-\lfloor b-a\rfloor_{s},b-\lfloor b-a\rfloor_{s},-z), & \text{if } z>0, \\ 
S+m\left (a,b-\lfloor b-a\rfloor_{s},z  \right ), & \text{if }z<0,\end{matrix}\right.
\end{equation*}
where we again applied the Kummer's identity for the negative case. A performance of such procedure is visualized in  \cite{watson::writeup1}. 

The two methods can be also combined if the only one evaluation of $m(\cdot)$ in the presented algorithm is evaluated using the bounds presented in the previous subsection. Clearly out of the possible cases that can appear under the Watson settings, the maximal error $\left (c_{3_{(1)}} -c_{3_{(2)}}   \right )\left ( 1+\frac{a\log(\frac{a}{b})}{b-a} \right )$ is at most $\approx 0.35$ and thus by taking the mean of the bounds the error is at most $\approx 0.1759$. 

This method is also implemented in the \pkg{watson} as the last case scenario. First using the algorithms and asymptotic formula implemented in ``gsl - scientific library", the code tries to evaluate the function directly. If the resulting error value is not 0 (i.e. GSL\_SUCCESS), the code tries to evaluate the transformed version using the Kummer's identity. If also this method fails (this is usually only for very big values, or parameters), the values are calculated using the presented bounds. Finally if the difference between the bounds is bigger than $2\log(1.02)$ (i.e., the true value of $M(a,b,z)$ could appear outside of the interval $[\theta/1.02,\theta*1.02]$, where $\theta$ is the calculated value), the above method is used which has not been observed to fail so far.  

\section{Application}

\subsection{Simulation study}

In this section we present and illustrate the package and its EM algorithm on the simulated data. Again we will concentrate on the case that can be visualized and so on the data with $p=3$. These are simulated using the \fun{rmwat()} function, 
<<echo=FALSE>>=
set.seed(1)
@
<<>>=
d <- rmwat(n = 2000, weights = c(0.1, 0.3, 0.2, 0.2, 0.2), 
           kappa = c(-200, -200, 30, 50, 100), 
           mu = matrix(c(1, 1, 1, -1, 1, 1, -1, -1, -1, 0, 1,
                                     -1, 1, 0, 0), nrow = 3))
@
\noindent where the parameters are chosen such that the clusters are overlapped while all of them being unique in the sense of the $\kappa$ and $\mu$ parameter. As can be observed from the function call, the true simulated mixture consists of 5 components distributions, where 2 components are of the great circle shape (i.e., with $\kappa<0$) and 3 with are concentrated in their mean direction.  


<<>>=
model <- watson(d, 7, minweight = .02, nruns = 50)
model
@

The model is estimated with the \fun{watson()} function, where the number of clusters is on purpose missspecified to 7. To allow the algorithm to converge to the true constellation, the \code{minweight} parameter is set to $2\%$, removing dynamically components with weight smaller than the given value. The procedure is repeated 50 times with different random initial values and the best model is stored. finally, the results are printed.

Even through the missspecification, similarly as in the first example, the algorithm converged to the true number of components. What is more, comparing the estimated parameters, one can easily recognize the simulated distribution. The fitted model consists of two clusters with huge negative $\kappa$ close to $-200$ and three components with positive $\kappa$ close to 30, 50 and 100, respectively, which are exactly the parameters used to simulate the distribution. The achieved likelihood is $3415.743$ with the average likelihood per data-point $1.7$, which is more than if the true assignments to components are specified ($3414.263$). Finally, the visualized results follow. 
<<eval=FALSE, echo=FALSE>>=
logLik(watson(d, ids = TRUE))[1]
@



<<eval=FALSE, echo=FALSE>>=
library(rgl)

rgl.open()
view3d( theta = 195, phi = 10, zoom =0.5)
rgl.bg(color = "white")
rgl.points(d[,1],d[,2],d[,3], ylim=c(-1,1),col = attr(d,"id") ,xlim=c(-1,1), zlim = c(-1,1), xlab = "x", ylab = "y", zlab = "z", alpha = 0.8)
spheres3d(x = 0, y = 0, z = 0, radius = 0.98, col = "yellow", alpha = 0.6, back = "lines")
rgl.lines(c(-1.2,1.2), c(0, 0), c(0, 0), color = "black")
rgl.lines(c(0, 0), c(-1.2,1.2), c(0, 0), color = "blue")
rgl.lines(c(0, 0), c(0, 0), c(-1.2,1.2), color = "red")
rgl.snapshot("~/Dropbox/Watson/writeup2/sim1.png")

colo = predict(model)*(-1)+6
colo = sapply(colo, function(x) if(x==4){5} else if(x==5){4} else x)
rgl.open()
view3d( theta = 195, phi = 10, zoom =0.5)
rgl.bg(color = "white")
rgl.points(d[,1],d[,2],d[,3], ylim=c(-1,1),col =  colo ,xlim=c(-1,1), zlim = c(-1,1), xlab = "x", ylab = "y", zlab = "z", alpha = 0.8)
spheres3d(x = 0, y = 0, z = 0, radius = 0.98, col = "yellow", alpha = 0.6, back = "lines")
rgl.lines(c(-1.2,1.2), c(0, 0), c(0, 0), color = "black")
rgl.lines(c(0, 0), c(-1.2,1.2), c(0, 0), color = "blue")
rgl.lines(c(0, 0), c(0, 0), c(-1.2,1.2), color = "red")
rgl.snapshot("~/Dropbox/Watson/writeup2/sim2.png")
@

\begin{figure}[h]
\hspace{0.4cm}\includegraphics[scale=0.59]{sim1.png}%
\hspace{0.8cm}\includegraphics[scale=0.59]{sim2.png}\\
\centerline{\footnotesize\hbox to 0.5\textwidth{\hfil
(a) True \hfil}\hbox to 0.5\textwidth{\hfil
(b) Estimated \hfil}}
\caption{Estimation results}
\end{figure}



\subsection{Supervised settings - New Zealand earthquake data}

To illustrate the usage of \pkg{watson} with real data in the supervised settings, we consider the New Zealand earthquake data, which were recently analysed by \cite{ArnoldJupp} and  \cite{BayesBingham}. Tectonic stress that occurs during the earthquake gives rise to a rapture on a fault plane (planar surface across which relative motion occurs, see \cite{EarthquakeBook}). It is of the interest of the geologists and geophysicists to compare these faults, and by doing so to compare the earthquakes at different times or locations to analyse the possible similarities. The geometry of these faults is commonly described by three angles: strike, dip and slip \citep{EarthquakeBook}, which can be transformed into a triplet of orthogonal axes known as the compressional, P, tensional, T, and null, B. This forms an orthogonal axial frame (set of $r$ orthogonal axes $\{u_1,u_2,\dots,u_r\}$ in $\mathbb{R}^p$) with $p=r=3$ describing the main directions of the focal mechanics. We note, that the frame is of an axial structure, because of the freedom in the choice of the reference wall. While such data can be treated as a 3-dimensional axial frame as in \cite{ArnoldJupp}, similarly as in \cite{BayesBingham}, we will dedicate our analysis only to the null axis, i.e., we will consider the data of the form $p=3$, $r=1$.

The dataset can be composed by merging three smaller datasets, each containing data from a different place or time. The first two datasets contain 50 observation each, near Christchurch which were observed before and after an earthquake on 22 February 2011 (labeled as CE and CL). The third dataset consists of 32 observation obtained from the South Island. Geophysicists are interested weather the event on 22 February 2011 changed the pattern of the earthquakes close to Christchurch and weather this structure is similar to the one found in South Island.

Considering now the code of the analysis, the dataset is firstly created by merging the smaller data-frames and the true factors are extracted. The angles are then transformed into the strike, dip and slip angles \citep{ArnoldBayes}, which are then used to generate the null axis for all the measurements \citep{EarthquakeBook}. Finally, the estimation for all three groups is performed where the true cluster assignment are given using the \code{ids} parameter. We note that the estimation can be performed also in a unsupervised settings, however because of the strong overlap that is present in the data, the estimation procedure generates cluster with much higher likelihood as the true assignments with however no connection to the true groups.   

{\scriptsize
<<>>=
dataa <- rbind(cbind(read.csv("~/Dropbox/Watson/Example/cantyearlyPTaxes.csv",header=T), type = "CE"),
               cbind(read.csv("~/Dropbox/Watson/Example/cantylatePTaxes.csv",header=T), type = "CL"),
               cbind(read.csv("~/Dropbox/Watson/Example/lewisPTaxes.csv",header=T), type = "L"))
classif <- as.factor(dataa$type)
dataa <- dataa[, -4]

ind = dataa$theta>=pi/2
dataa[ !ind, ] <- cbind(dataa[ !ind, 1] + pi, pi - dataa[ !ind, 2], pi - dataa[ !ind, 3])
new_angles <- as.data.frame(cbind(strike = dataa$phi-pi/2,
                                  dip = pi - dataa$theta,
                                  rake = dataa$psi + pi/2))

b <- cbind(-sin(new_angles$rake)*cos(new_angles$strike) + 
             cos(new_angles$rake)*cos(new_angles$dip)*sin(new_angles$strike),
           sin(new_angles$rake)*sin(new_angles$strike) + 
             cos(new_angles$rake)*cos(new_angles$dip)*cos(new_angles$strike),
           cos(new_angles$rake)*sin(new_angles$dip))

gg <- watson(b, ids = classif)
gg
@
}

 HERE ADD TEST IF THIS STAYS BECAUSE THIS IS TOO AD-HOC

The results suggest similar observations as were obtained in the previous literature. The clusters next to Christchurch do not show any strong evidence in the difference of the estimated parameters. Both clusters have a concentration parameter $\kappa$ close to 4 with the Euclidean norm of the mean direction difference equal to  $\approx 0.055$. The authors in  \cite{ArnoldJupp} obtained in their $p=r=3$ setting from a test on the equality p-value of 0.89, which goes along the lines of the results visible here. 

Comparing the results from the first Christchurch cluster with the South Island results indicates a much stronger difference as between the Christchurch datasets. The concentration of the earthquakes is much smaller in the case of South Island and the euclidean distance between the mean directions of first Christchurch cluster and South Island cluster  is $\approx 0.49$. For this comparison,  \cite{ArnoldJupp} obtained p-value smaller than 0.001, which again coincides with the observations obtained here.    

\subsection{Unsupervised settings - Depth image clustering}

The expanding use of cameras in a daily life and also in many scientific disciplines attracted a decent amount of attention to the research areas like image processing, robotics or computer vision. These interests have even gone bigger together with the capabilities of the cameras to produce a high quality color images. However, while the quality of images is still on an increasing path, the amount of information that can be extracted from such a projection of a 3D space on a 2D space has its natural bound. To overcome this limitations, some cameras (mostly in the gaming and scientific industries, e.g., Microsoft Kinect sensors) have integrated sensors to explore the geometric structure of the surrounding using the so called depth images. The depth image assigns to every pixel the values that represent the distance of the object from a viewpoint. While the amount of information that these images contain is also limited, the depth images excel in recognition of long planar surfaces, which is many times a difficult task if only an RGB picture is analysed, see \cite{PhdHasnat}. This allows to use the depth images and the features extracted from it as additional components in the analysis of the color images, which together forms the  RGB-D image analysis. 

The recognition of the planar surfaces is commonly deployed using the surface normal (see e.g. \citep{NYUData}), which is a 3-dimensional unit vector representing the normal to the pixel of interest and the pixels in the neighborhood that fall under the prespecified threshold when comparing the depth values. The sample space of these normals is a 3D-Sphere, which makes the distributions from the directions statistics a great choice when analyzing these vectors. Even better, due to its axial symmetry, the Watson distribution can better handle the noise in the surface normals \citep{Rusu}, making it a superior choice over the usual choices as for example the von Mises-Fisher. 

To illustrate the usage of \pkg{watson} in the unsupervised settings, we consider the NYU Depth Dataset V2 \citep{NYUData}, which consists of a collection of color and depth images obtained using the Microsoft Kinect cameras. The surface normals were extracted using the toolbox of the NYU database. In fact, the Watson distribution has been already used to cluster this database (see \cite{WatsonDepth}), however the model were estimated with a slightly different procedure when the hierarchical clustering was applied to generate the submodels. In this example we illustrate how easily can the surface normals be clustered using the \pkg{watson} package, allowing the R users to process these types of images just by using few lines of code.

First of all the additional packages are loaded. Packages \pkg{grid} and \pkg{viridis} will be used to visualize the results  while the \pkg{parallel} package is loaded in order to estimate the models with different number components in parallel. After the data are loaded the \fun{grid.raster()} function is used to generate an image representing the depth image on a red scale. Each picture from the NYU Depth Dataset V2 consists of $427\times 561$ pixels, giving together 239547 surface normals per picture to analyse.

<<eval=FALSE>>=
library(grid)
library(viridis)
library(parallel)

num <- 451
pasnum <- sprintf("%06d", num)
depth <- read.csv(paste0("~/Documents/Watson/example/depthcsv/
                         depth_",pasnum,".csv"), header = F)
norm <- read.csv(paste0("~/Documents/Watson/example/
                        surface_normals_csv/surface_normals_",pasnum,
                        ".csv"), header = F)

pdf(paste0("~/Documents/Watson/example/resultpic/pic_",pasnum,".pdf"),
    width=6, height=4)

r <- as.matrix((255/(max(depth-min(depth))))*(depth-min(depth)))
g <- matrix(0, 427,561)
b <- matrix(0, 427,561)
col <- rgb(r, g, b, maxColorValue = 255)
dim(col) <- dim(r)
grid.newpage()
grid.raster(col, interpolate=FALSE)
@

The surface normals are then clustered in parallel using 3 parallel units for models with number of components ranging from 2 to 7. Each estimation performs 100 runs, after which the run with best likelihood is returned (we note that from our observations a much smaller number of runs is needed for this data to obtain similar results, and the 100 runs were used more as a hedging against the possible unlucky case than from necessity). Additionally, the parameter minweight is again specified to deny clusters with weight smaller than 0.05.  After the estimation, the data-set is erased from all the models to avoid overflow on memory if multiple models for multiple pictures are stored. Next the categories for all pixels are predicted, the results are visualized and BIC scores are extracted. This procedure is then repeated with the E argument equal to \code{"hardmax"}, indicating the algorithm to perform the hard-clustering. Finally, the plot with BIC scores is stored.   

<<eval=FALSE>>=
a <- as.matrix(norm)

watrun <- function(i, a){
  watson(a, k = i, minweight = 0.05, nruns = 100, verbose = T)
  w$data <- NULL
  w
}

watrunhard <- function(i, a){
  w = watson(a, k = i, E = "hardmax", minweight = 0.05, 
             nruns = 100, verbose = T)
  w$data <- NULL
  w
}

cl <- makeCluster(3, outfile = "progress.txt")
clusterExport(cl = cl, list("watson"))
parallel::clusterSetRNGStream(cl = cl, 1)
B <- parLapply(cl, c(2, 7, 4, 5, 6, 3),
               watrun, a)[c(1, 6, 3, 4, 5, 2)]
stopCluster(cl)

BIC_S <- unlist(lapply(B, function(w){
  i <- length(w$kappa_vector)
  unif <- which(abs(w$kappa_vector)==min(abs(w$kappa_vector)))
  mag <- magma(i)
  e <- mag[unif]
  mag[unif] <- mag[1]
  mag[1] <- e
  r <- matrix(mag[predict(w)], 427, 561)
  grid.newpage()
  grid.raster(r, interpolate=FALSE)
  BIC(w)
} ))


cl <- makeCluster(3, outfile = "progress.txt")
clusterExport(cl = cl, list("watson"))
parallel::clusterSetRNGStream(cl = cl, 1)
B <- parLapply(cl, c(2,7,4,5,6,3),
               watrunhard, a)[c(1,6,3,4,5,2)]
stopCluster(cl)

BIC_H <- unlist(lapply(B, function(w){
  i <- length(w$kappa_vector)
  unif <- which(abs(w$kappa_vector)==min(abs(w$kappa_vector)))
  mag <- magma(i)
  e <- mag[unif]
  mag[unif] <- mag[1]
  mag[1] <- e
  r <- matrix(mag[predict(w)], 427, 561)
  grid.newpage()
  grid.raster(r, interpolate=FALSE)
  BIC(w)
} ))

plot(2:7, BIC_S, ylim = range(c(BIC_S,BIC_H)))
points(2:7, BIC_H, pch = 4)
dev.off()
@
\begin{figure}[htb]
    \begin{minipage}[t]{.05\textwidth}
        \centering
        \vspace{1cm}
        { RGB \\ + \\ S } \\
        \vspace{1.5cm}
        {D\\ + \\ H } \\
        \vspace{1cm}
        { \ } \\
        \vspace{1cm}
        {S } \\
        \vspace{2.5cm}
        {H} \\
        %\subcaption{\hspace*{0.4cm} RGB }
    \end{minipage}
    \begin{minipage}[t]{.28\textwidth}
        \centering
        { \hspace*{0.4cm} \ } \\
        \includegraphics[page= 1,width=1.25\textwidth]{pic_000451.pdf}\\
        \includegraphics[page= 2,width=1.25\textwidth]{pic_000451.pdf}\\
        {\bf \hspace*{0.6cm} $k=4$ } \\
        \includegraphics[page= 6,width=1.25\textwidth]{pic_000451.pdf}\\
        \includegraphics[page= 12,width=1.25\textwidth]{pic_000451.pdf}
        %\subcaption{\hspace*{0.4cm} RGB }
    \end{minipage}
    \hspace{0.2cm}
    \begin{minipage}[t]{.28\textwidth}
        \centering
        {\bf \hspace*{0.6cm} $k=2$} \\
        \includegraphics[page= 4,width=1.25\textwidth]{pic_000451.pdf}\\
        \includegraphics[page= 10,width=1.25\textwidth]{pic_000451.pdf}\\
        {\bf \hspace*{0.6cm} $k=5$ } \\
        \includegraphics[page= 7,width=1.25\textwidth]{pic_000451.pdf}\\
        \includegraphics[page= 13,width=1.25\textwidth]{pic_000451.pdf}
        %\subcaption{\hspace*{0.4cm} RGB }
    \end{minipage}
    \hspace{0.2cm}
    \begin{minipage}[t]{.28\textwidth}
        \centering
        {\bf \hspace*{0.6cm} $k=3$}\\
        \includegraphics[page= 5,width=1.25\textwidth]{pic_000451.pdf}\\
        \includegraphics[page= 11,width=1.25\textwidth]{pic_000451.pdf}\\
        {\bf \hspace*{0.6cm} $k=6$ } \\
        \includegraphics[page= 8,width=1.25\textwidth]{pic_000451.pdf}\\
        \includegraphics[page= 14,width=1.25\textwidth]{pic_000451.pdf}
        %\subcaption{\hspace*{0.4cm} $k=5$}
    \end{minipage}
    \caption{The figure shows the estimate results. First two picture show the true RGB picture (not used in the analysis) and the depth picture from which the surfaces were estimated. The black color has been always assigned to the components with the smallest concentration in the absolute value, i.e. the component that collects the elements that do not belong to the dominant clusters.}
\end{figure}
 The picture indicates, unsurprising behavior. With a small number of components the algorithm tends to pick the dominant surfaces in the picture and concentrate on them. In our observations, the resulting parameter setup always tend to have one cluster with small concentration parameter, which thus behaves as a uniform distribution and hence clusters the elements that have not been assigned to the dominant clusters in some specific directions. This results has been also observed in \cite{PhdHasnat}. In our analysis the black color was always assigned to this cluster, while the other colors were assigned by random. 

\begin{figure}[h]
\centering
\includegraphics[page= 16,width=1\textwidth]{pic_000451.pdf}
\caption{BIC scores of the models ranging from $k=2$ to $k=7$. Circles indicate the scores in the case of the soft-assignment and crosses in the case of hard-assignment.}
\end{figure}

The BIC scores can be then again used to detect the optimal number of components using the kink plot. Similarly as with other pictures the hard and soft clustering tends to agree on likelihood up to the point where the models start to overfit. Here by observing the pictures the hard-assignment performs slightly better as it tends to use more the ability to remove non-significant clusters and hence to climb down to the true number of planar surfaces. For the image analyzed above the BIC scores and also the attached pictures indicate that after $k=4$ the models tends to overfit and fails to perfectly recognize the planar surfaces as one element. For more discussion on the selection of the optimal $k$, see \cite{PhdHasnat}. 
\newpage


\begin{figure}[htb]
    \begin{minipage}[t]{.15\textwidth}
        \centering
        {\bf \hspace*{0.4cm} RGB} \\
        \includegraphics[page= 1,width=1.25\textwidth]{pic_000450.pdf}\\
        \includegraphics[page= 1,width=1.25\textwidth]{pic_000977.pdf}\\
        \includegraphics[page= 1,width=1.25\textwidth]{pic_000723.pdf}\\
        \includegraphics[page= 1,width=1.25\textwidth]{pic_000211.pdf}\\
        \includegraphics[page= 1,width=1.25\textwidth]{pic_000726.pdf}
        %\subcaption{\hspace*{0.4cm} RGB }
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.15\textwidth}
        \centering
        {\bf \hspace*{0.4cm} D} \\
        \includegraphics[page= 2,width=1.25\textwidth]{pic_000450.pdf}\\
        \includegraphics[page= 2,width=1.25\textwidth]{pic_000977.pdf}\\
        \includegraphics[page= 2,width=1.25\textwidth]{pic_000723.pdf}\\
        \includegraphics[page= 2,width=1.25\textwidth]{pic_000211.pdf}\\
        \includegraphics[page= 2,width=1.25\textwidth]{pic_000726.pdf}
        %\subcaption{\hspace*{0.4cm} Depth }
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.15\textwidth}
        \centering
        {\bf \hspace*{0.4cm} $k=2$} \\
        \includegraphics[page= 10,width=1.25\textwidth]{pic_000450.pdf}\\
        \includegraphics[page= 10,width=1.25\textwidth]{pic_000977.pdf}\\
        \includegraphics[page= 10,width=1.25\textwidth]{pic_000723.pdf}\\
        \includegraphics[page= 10,width=1.25\textwidth]{pic_000211.pdf}\\
        \includegraphics[page= 10,width=1.25\textwidth]{pic_000726.pdf}
        %\subcaption{\hspace*{0.4cm} $k=2$}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.15\textwidth}
        \centering
        {\bf \hspace*{0.4cm} $k=3$} \\
        \includegraphics[page= 11,width=1.25\textwidth]{pic_000450.pdf}\\
        \includegraphics[page= 11,width=1.25\textwidth]{pic_000977.pdf}\\
        \includegraphics[page= 11,width=1.25\textwidth]{pic_000723.pdf}\\
        \includegraphics[page= 11,width=1.25\textwidth]{pic_000211.pdf}\\
        \includegraphics[page= 11,width=1.25\textwidth]{pic_000726.pdf}
        %\subcaption{\hspace*{0.4cm} $k=3$}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.15\textwidth}
        \centering
        {\bf \hspace*{0.4cm} $k=4$} \\
        \includegraphics[page= 12,width=1.25\textwidth]{pic_000450.pdf}\\
        \includegraphics[page= 12,width=1.25\textwidth]{pic_000977.pdf}\\
        \includegraphics[page= 12,width=1.25\textwidth]{pic_000723.pdf}\\
        \includegraphics[page= 12,width=1.25\textwidth]{pic_000211.pdf}\\
        \includegraphics[page= 12,width=1.25\textwidth]{pic_000726.pdf}
        %\subcaption{\hspace*{0.4cm} $k=4$}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.15\textwidth}
        \centering
        {\bf \hspace*{0.4cm} $k=5$} \\
        \includegraphics[page= 13,width=1.25\textwidth]{pic_000450.pdf}\\
        \includegraphics[page= 13,width=1.25\textwidth]{pic_000977.pdf}\\
        \includegraphics[page= 13,width=1.25\textwidth]{pic_000723.pdf}\\
        \includegraphics[page= 13,width=1.25\textwidth]{pic_000211.pdf}\\
        \includegraphics[page= 13,width=1.25\textwidth]{pic_000726.pdf}
        %\subcaption{\hspace*{0.4cm} $k=5$}
    \end{minipage}
    \caption{Estimated results using the hard-clustering for the other selected pictured.}
\end{figure}

\newpage
\section*{Acknowledgement}

The authors are grateful to Richard Arnold and Peter Jupp for providing the New Zealand earthquake data.

\bibliographystyle{plainnat}
\bibliography{watson.bib}

\end{document}
