\documentclass[a4paper]{article}
\usepackage{amsfonts,amsthm}
\usepackage{amsmath, enumerate, bm}
\usepackage[round]{natbib}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{bm}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\P}{\mathbb{P}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{prop}{Proposition}
\newcommand{\pkg}[1]{\textbf{\textsf{#1}}}
\newcommand{\class}[1]{\textmd{\textsl{#1}}}
\newcommand{\fun}[1]{\texttt{#1}}
\DeclareMathOperator*{\argmax}{arg\,max} 
\def\code#1{\texttt{#1}}
\title{watson: An R Package for Fitting Mixtures of Watson Distributions}

<<include=FALSE>>=
#setwd('/home/sablica/Documents/Polytomous/writeup')
Sys.setenv(TEXINPUTS=getwd(),
           BIBINPUTS=getwd(),
           BSTINPUTS=getwd())
@

\begin{document}

\maketitle
\section{Introduction}

In the history of the mankind the clustering on the sphere have long gained a lot of popularity, while still beeing a difficult task mostly when it comes to estimation. The estimation troubles usually come out from the difficult to evaluate normalising constant associated with the given directional distribution. These normalising terms can be for many most used directional distributions represented in terms of a inifinite hypergeometric series, and thus now allowing to obtain any closed, or easy to evaluate form.

Probably the most natural choice for the directional distribution is the von Mises-Fisher (vMF) distribution.  The
vMF distribution has concentric contour lines similar to the multivariate normal distribution which allow to model data concentrated around these directions. Here the estimating procedure has been solved by introducing tight bounds on the normalising constant  \cite{watson::Hornik2014E} that allowed to build a fitting framework for mixtures of such distributions \cite{watson::Hornik2014P}.  

While the vMF distribution is the first choice for most of the directional data, the distribution can be inappropriate if the data contain some additional structure. An example of such data are the axially symmetric data (i.e., $x$ and $-x$ are indistinguishable) used for example in structural geology or rock magnetism areas. An essential choice in such cases is the Watson distribution \cite{watson::Watson1965} whose mixture modelling will be of the main focus in this paper.

Recently, mostly thanks to the renewed attention in drectional data modeling thanks to maschine learning and sentiment analysis, the Watson distribution and its application in mixture modelling was discussed in \cite{watson:Bijral+Breitenbach+Grudic:2007} and  \cite{watson:Sra+Karp:2013}. The approximation of the maximum likelihood estimation using continued fractions shows anomalous convergence for a major set of values producing the loss of numerical precision, for more details see \cite{watson:Gautschi:1977}. On the other hand, the authors in \cite{watson:Bijral+Breitenbach+Grudic:2007} derived an maximum likelihood estimation using useful approximations but witohut any mathematical justification. A brilliant and mathematically justified approach is to use the derived bounds from  \cite{watson:Sra+Karp:2013} to approximate the true values during the estimation, which however again comes with the price of an approximation error. 

A new wind to this problematics was presented in \cite{watson::writeup1}, where the authors derived an convergent sequence of bounds that can be used during the estimation procedure. These bounds only after two iterations provide a state of the art boundaries and allow to estimate the likelihood with arbitary precision. This and also other numerical techniques and applications from the above paper will be used and adjust to the modelling of the axial data using Watson distribution which is ofered as an R-packge \pkg{watson}. 


\section{The Watson distribution}
Following the notation in \cite{watson:mardia:2009}, let $\mathbb{S}^{p-1} = \{x\in\mathbb{R}^{p},\left \| x \right \|=1 \}$ be an $(p-1)$ dimensional unit sphere with the projective hyperplane $\mathbb{P}^{p-1}$ such that the symmetric vectors $\pm x\in\mathbb{S}^{p-1}$ are equivalent. A random unit length vector in $\mathbb{R}^p$ has a Watson distribution with concentration parameter $\kappa \in \mathbb{R}$ and mean direction parameter $\mu \in \mathbb{S}^{p-1}$, if its density with respect to the uniform distribution on the unit sphere is
$$f(x|\kappa ,\mu)=M\left (\frac{1}{2},\frac{p}{2},\kappa  \right )^{-1}e^{\kappa\left ( x^{t}\mu \right )^2}$$
where
$$M(\alpha, \beta, z) ={{}_1F_1}(\alpha; \beta; z) = \sum_{n=0}^\infty \frac{(\alpha)_n}{(\beta)_n}\frac{z^n}{n!} $$
is the confluent hypergeometric function of the first kind, also known as Kummer's function, solving the Kummer's differential equation 
$z w'' + (\beta - z) w' - \alpha w = 0.$ 

We note that if $\kappa>0$ the density has maxima at $\pm\mu$ and so is bipolar. On the other hand for $\kappa<0$, the distribution is concentrated around the great circle orthogonal to $\mu$, and so the distribution is a symmetric girdle distribution. As $\kappa$ goes to $\pm\infty$ the distribution becomes more and more concentrated around $\mu$ if $\kappa\rightarrow\infty$ and around the orthogonal circle if $\kappa\rightarrow-\infty$. Finally, observe that for orthogonal $Q$ s.t. $Q\mu=\mu$, it holds that $\mu^{t}(Qx)=\mu^{t}x$, implying the rotationall symmetry about $\mu$. For more details, see \cite{watson:mardia:2009}.

\subsection{Simulation of the Watson distribution}

\subsubsection{Algorithm structure}
We are interested in a random sample generation from a Watson distribution. For the following denote the Watson distribution as as 
$$ f_{wat}(x|\kappa ,\mu)=c_{wat}f_{wat}^{*}(x|\kappa ,\mu)=c_{wat}e^{\kappa\left ( x^{t}\mu \right )^2},$$
where $c_{wat}$ is the normalizing constant. %For more details see \cite{watson:mardia:2009}. 
For the purpose of clarity, the sampling procedure will be splitted and investigated in three mutualy exclusive cases.

Firstly, assume $\kappa=0$. The watson distribution then simplifies to the uniform or isotropic distribution on sphere $S^{p}$, \cite{watson:bingham:1974}, which can be easily generated using $p$ standart normal random variables. Assume $X_{i}\sim N(0,1)$ and $X =(X_{1},\dots,X_{n})$, then $Y = X/\left \| X \right \|$ has a uniform distribution on a sphere $S$. To see this note that $X\sim N_{n}(0,I_{n})$ is invariant under rotations, i.e., $QX\stackrel{d}{=}X$ for any orthogonal $Q$, and since $\left \| Y \right \|=1$, $Y$ must be uniformly distributed on the sphere. 

For the cases of non-zero $\kappa$ is the current literature that considers directly the Watson distribution, up to our best knowledge, equiped only with the cases where $p\leq 3$, for example see \cite{watson:Best:1986} or \cite{watson:hung:1993}. Interestingly however, a slightly more general answer to the question was published by \cite{watson:Kent}, where the authord derive an BACG rejection sampling method for the Bingham distribution which generalizes the Watson one. As will be shown later, appling this theory directly only to the Watson distribution not only provides an sampling algorithm, but also allows to calculate the optimal hyper-parameters analytically, which is not the case for general Bingham distribution.

The Bingham distribution is an symmetric probability distribution mainly used for modeling of axial data on the sphere $S^{p}$. It is a generalization of the Watson distribution and a special case of the Kent and Fisher-Bingham distributions. The density function is of the form $$f_{bing}(x|A)=c_{bing}f_{bing}^{*}(x|A)=c_{bing}e^{ -x^{t}Ax }$$ which can be obtained by th Watson distribution as $$ f_{Wat}(x|\kappa ,mu)=c_{Wat}e^{\kappa\left ( x^{t}\mu \right )^2}=c_{Wat}e^{x^{t}\overbrace{(-\kappa\mu\mu^t)}^{A}x}.$$ The minus in the exponential is not neccesary but it allows a simplifications in the further deviations. Additionally, recall that the Bingham distribution with parameter matrix $A+cI_{p}$ defines the same distribution for any $c\in\mathbb{R}$. This allows without any loss of generality to assume that the eigenvalues of $A$ satisfy $\min_{i=0,\dots,p}\lambda_{i}=0$, which will be also assumed in the upcoming steps.

Following \cite{watson:Kent}, let $b>0$ and defining  $u=x^{t}Ax$ and $\Omega = I+2A/b,$ the envolope inequality for the Bingham distribution is then
$$ f_{Bing}^{*}(x)\leq e^{-(q-b)/2}(q/b)^{q/2}f_{ACG}^{*}(x),$$
where $f_{ACG}^{*}(x)=\left(x^{t}\Omega x \right)^{-q/2}$ is the unnormalized density of angular central Gaussian distribution. Recall that if $Y\sim N_{p}(0,\Sigma)$ then $Y/\left \| Y \right \|\sim ACG(\Omega)$ with $\Omega=\Sigma^{-1}$. Thus by simulating $U\sim U(0,1),$ $X\sim N_{p}(0,\Sigma)$, defining $Y=X/\left \| X \right \|$ we accept $Y$ if 
$$U <\frac{f_{Bing}^{*}\left (Y  \right )}{M\left ( b \right )f_{ACG}^{*}\left ( Y \right )}=\frac{e^{ -y^{t}Ay } }{e^{-(q-b)/2}(q/b)^{q/2}\left(y^{t}\Omega y \right)^{-q/2}}, $$
or equivalently if
\begin{equation} \label{rejection}
\log(U) <  -y^{t}Ay +(q-b)/2 -(q/2)\log(q/b) +(q/2)\log\left(y^{t}\Omega y \right).
\end{equation}

Appliing the theory to the Watson distribution, first assume a negative $\kappa$. The following procedure generates random vector from Watson distribution with $\kappa<0$ and $\mu=\left(0,0,\dots,1\right)$. It is easy to observe that the parameter matrix $A$ is then of the form
$$A=\begin{pmatrix}
0 & \hdots &0 \\ 
\vdots & \ddots & \vdots\\ 
0 &\hdots  & -\kappa
\end{pmatrix}$$ and thus satisfies the assumption of non-negative eigenvalues. From this we can conclude that $$ 
\Sigma =\begin{pmatrix}
1 &0  & \hdots  \\ 
0 & \ddots & \vdots \\ 
\vdots & \hdots &b/(b-2\kappa)  
\end{pmatrix}.
$$ Thus by simulating $p-1$ random variables $X_{i}\sim N(0,1)$ for $i=1,\dots,p-1,$ and one $X_{p}\sim N(0,b/(b-2\kappa))$, $X=\left(X_{1},X_{2},\dots,X_{p} \right)$ and $Y=X/\left \| X \right \|$. Since $A$ contains only one nonzero element and $\Omega$ is diagonal, also the expression \ref{rejection} simplifies significantly.

The authors in \cite{watson:Kent} state a unique efficiency maximizer for the parameter $b$ given as a solution of $$ 
\sum_{i=1}^{p}\frac{1}{b+2\lambda_{i}}=1,$$ where $\lambda_{i}$ is the $i$-th eigen value of $A$. Since only one eigen value is non-zero this can be written as $$1= \frac{\left (p-1  \right )}{b}+\frac{1}{b-2\kappa}=\frac{-2\left (p-1  \right )\kappa+pb}{b\left ( b-2\kappa \right )}$$ which gives the quadratic equation $b^2+b\left ( -2\kappa-p \right )+2\left (p-1  \right )\kappa=0$ with the bigger roor equal to $$
b=\frac{1}{2}\left ( p+2\kappa+\sqrt{p^2+4\left ( \kappa+2 \right )\kappa -4p\kappa} \right ) .$$ Clearly the expression in square root and solution are non-negative if $p\geq 1$ which is always the case.

Assuming now $\kappa>0$ and $\mu=\left(0,0,\dots,1\right)$, using the invariance under spherical shift to obtain positive eigen values $A$ and $\Sigma$ can be written as $$ 
A=\begin{pmatrix}
\kappa & 0&\hdots &0 \\
0 & \ddots& \ddots& \vdots\\  
\vdots & \ddots & \ddots&\vdots\\ 
0 &\hdots  & \hdots& 0
\end{pmatrix} \quad \text{and} \quad \Sigma=\begin{pmatrix}
b/(b+2\kappa)  & 0&\hdots &0 \\
0 & \ddots& \ddots& \vdots\\  
\vdots & \ddots & \ddots&\vdots\\ 
0 &\hdots  & \hdots& 1
\end{pmatrix}.
$$ This again simplifies the  expression \ref{rejection} as the required matrices are diagonal. The optimal $b$ can be again obtian implicitly as a solution of $b^2+b\left ( 2\kappa-p \right )-2\kappa=0,$ which has the bigger root equal to $$b=\frac{1}{2}\left ( p-2\kappa+\sqrt{p^2+4 \kappa^{2} -4\kappa\left (p-2  \right )} \right ) .$$ Again the square root and $b$ are non-negative if $p\geq 1$.

To get samples for an arbitrary direction parameter $\mu$, the solution is multiplied
from the left with a $p\times p$ matrix  that consists of first $(p-1)$ columns forming an unitary basis of
the subspace orthogonal to $\mu$, obtainable via QR decomposition, and the last column is equal to $\mu$. 

\subsubsection{Software}
The software implementation of the above theory can be obtained using the \fun{rmwat()} function from the \pkg{watson} package. 
<<echo=FALSE>>=
cat("R> rmwat(n, weights, kappa, mu, b = -10)")
@ 
The arguments for this function are as follows.
\begin{description}
\item[\normalfont \code{x}:] an integer giving the number of samples to draw.
\item[\normalfont \code{weights}:] a numeric vector with non-negative elements giving the mixture probabilities.
\item[\normalfont \code{kappa}:] a numeric vector giving the kappa parameters of the mixture components.
\item[\normalfont \code{mu}:] a numeric matrix with columns giving the mu parameters of the mixture components.
\item[\normalfont \code{b}:] a positive numeric hyper-parameter used in the sampling. If not a positive value is given, optimal choice of b is used, default: -10.
\end{description}

\begin{figure}[ht]
\includegraphics[scale=0.23]{sphere1.png}%
\includegraphics[scale=0.23]{sphere2.png}\\
\centerline{\footnotesize\hbox to 0.5\textwidth{\hfil
(a) $\kappa = (20,20)$ \hfil}\hbox to 0.5\textwidth{\hfil
(b) $\kappa = (-200,-200)$ \hfil}}
\caption{Resulting samples}
\end{figure}
\noindent
{\bf Warning:} Due to the speed of simulation, the underlyning C++ code does not contain any checks that would interupt the simulation if the user would try to do so. This is in general safe as with the optimal parameter $b$ (derived above) the acceptence probability is close to $100\%$ (and increases with dimension). However, different choices of $b$ may cause difficulties in acceptance and are not recomended mostly for big dimensions unless you know what you are doing! The optimal $b$ is used if the input value is not suitable, i.e., is not positive (also the default case).

\subsection{Estimation of the Watson distribution} \label{sec2}
Let $x_{1},\dots,x_{n}\in \mathbb{P}^{p-1}$ be i.i.d. sample from the Watson distribution with parameters $\kappa$ and $\mu$ and define ${\bf X}$ to be the designe matrix with $x_{1},\dots,x_{n}$ as rows. The log-likelihood is then given by 
\begin{equation} \label{loglik}
\ell\left ( \kappa,\mu|x_{1}\dots x_{n} \right )= n(\kappa\mu^{t}{\bf S}\mu-\log\left ( M(1/2,p/2,\kappa) \right )+c),
\end{equation}
where ${\bf S}$ is the scatter matrix ${\bf S}={\bf X}^{t}{\bf X}/n $  and $c$ is a constant term that can be omitted. Since ${\bf S}$ is symmetric, $\mu^{t}{\bf S}\mu = R({\bf S},\mu)$ is a Rayleigh quotient of matrix ${\bf S}$ and thus satisfies $\lambda_{1}\leq R({\bf S},\mu) \leq \lambda_{p}, \forall \mu \in \mathbb{S}^{p-1}$, where $\lambda_{1}\leq\lambda_{2}\dots\leq\lambda_{p}$ are the ordered eigenvalues of ${\bf S}$. More preciselly, it holds that $R({\bf S},s_{p})=\lambda_{p}$ and $R({\bf S},s_{1})=\lambda_{1}$, where $s_{1},\dots, s_{p}$ are the normalised eigenvectors corresponding to $\lambda_{1},\dots,\lambda_{p}$. Hence maximizing \ref{loglik} with respect to $\mu$ gives obviously
\begin{equation} \label{mumax}
\hat{\mu}= \left\{\begin{matrix}
s_{1}, & \text{if } \hat{\kappa}<0, \\ 
s_{p}, & \text{if } \hat{\kappa}>0.\\ 
\end{matrix}\right.
\end{equation}
Taking the first order derivative with respect to $\kappa$ gives 
\begin{equation*} 
g\left(\frac{1}{2},\frac{p}{2}, \hat{\kappa}\right)=\frac{M'(\frac{1}{2},\frac{p}{2},\hat{\kappa})}{M(\frac{1}{2},\frac{p}{2},\hat{\kappa})}=\mu^{t}{\bf S}\mu=R({\bf S},\mu)=r,
\end{equation*}
with $0\leq \lambda_{1}\leq r\leq \lambda_{p}\leq 1, \ \forall \mu \in \mathbb{S}^{p-1} $ and $$g(\alpha, \beta, \kappa) = \frac{\mathrm{d} \log(M(\alpha, \beta,  \kappa)))}{\mathrm{d}  \kappa} =  \frac{{M(\alpha, \beta,  \kappa)}'}{M(\alpha, \beta,  \kappa)} = \frac{\alpha}{\beta}\frac{{M(\alpha+1, \beta+1,  \kappa)}}{M(\alpha, \beta,  \kappa)},$$ as defined in \cite{watson:Sra+Karp:2013} or \cite{watson::writeup1} yields the problem that has to be solved. Clearly the maximizations are not independent, however if $\hat{\kappa}>0,$ $s_{p}$ and $\lambda_{p}$ are optimal and if $\hat{\kappa}<0$, the solution is given by $s_{1}$ and $\lambda_{1}$. Thus one can solve $g\left(\frac{1}{2},\frac{p}{2}, \hat{\kappa}\right)=\lambda_{1}$ and $g\left(\frac{1}{2},\frac{p}{2}, \hat{\kappa}\right)=\lambda_{2}$ and pick the combination with higher likelihood. 

Sumarizing, we are interested in the solution $\kappa$ of the highly-nonlinear problem 
\begin{equation} \label{kappasol}
g\left(\alpha,\beta, \kappa\right)=r, \quad \text{where} \quad 0<\alpha<\beta, \text{ and } 0\leq r\leq 1.
\end{equation}
It can be shown (see \cite{watson:Sra+Karp:2013}) that $g(\alpha, \beta, z)$, for the case of Watson distribution, is a strictly increasing function which maps the interval $(-\infty,\infty)$ onto the interval $(0, 1)$. Even though $g(\alpha, \beta, z)$ admits a continued fraction representation, possible approximation using continued fractions shows anomalous convergence for a major set of values producing the loss of numerical precision, for more details see \cite{watson:Gautschi:1977}.

\cite{watson:Sra:2007} suggested the ad-hoc approximation
\begin{equation} \label{sra2007}
\kappa \approx \left ( \alpha +\beta -1 \right )\left ( \frac{1}{1-r}-\frac{\alpha }{(\beta -1)r} \right ),
\end{equation}
based on the approximation of $M(a,b,z)\approx M(a,b+1,z)$. 

\cite{watson:Bijral+Breitenbach+Grudic:2007} in around the same time ofered another approximation 
\begin{equation} \label{BBG}
\kappa \approx \frac{\beta r-\alpha  }{r\left ( 1-r \right )},
\end{equation}
which they observed to be accurate for the Watson case. The equation was derived from the assumption $g(a,b,\kappa)\approx g(a+1,b+1,\kappa)$ and was also ofered together with a correction term (``determined empirically"). The final form was presented as
\begin{equation} \label{BBG_c}
\kappa \approx \frac{\beta r-\alpha  }{r\left ( 1-r \right )}+\frac{r}{2\beta\left ( 1-r \right )}.
\end{equation}

A huge step further was then accomplished in \cite{watson:Sra+Karp:2013}, where the authors derived tight bounds for the inverse of $g(\alpha, \beta, \kappa)$. Defining, 
\begin{align} \label{bounds}
\begin{split}
L(r)=&\frac{r\beta-\alpha}{r(1-r)}\left ( 1+\frac{1-r}{\beta-\alpha} \right ),\\
B(r)=&\frac{r\beta-\alpha}{2r(1-r)}\left ( 1+\sqrt{1+\frac{4\left ( \beta +1 \right )r\left ( 1-r \right )}{\alpha\left (\beta-\alpha  \right )}} \right ),\\
U(r)=&\frac{r\beta-\alpha}{r(1-r)}\left ( 1+\frac{r}{\alpha} \right ),
\end{split}
\end{align}
it has been shown that the solution of equation \ref{kappasol} satisfies
\begin{align*} 
\begin{split}
&L(r)<\kappa<B(r)<U(r) \quad \text{for }0<r<\alpha/\beta, \\
&L(r)<B(r)<\kappa<U(r) \quad \text{for }\alpha/\beta<r<1,
\end{split}
\end{align*}
where all bounds are additionally exact at $\kappa = 0$, i.e., $r=\alpha/\beta$. To have a unanimous decision rule which bound to use, the authors suggest the following rule-of-thump:
\begin{equation} \label{sra2013}
\kappa \approx \left\{\begin{matrix}
U(r), & \text{if}& 0<r<\frac{\alpha}{2\beta},\\ 
B(r), & \text{if}& \frac{\alpha}{2\beta}\leq r<\frac{2\alpha}{\sqrt{\beta}},\\ 
L(r), & \text{if}& \frac{2\alpha}{\sqrt{\beta}}\leq r<1.
\end{matrix}\right. \\
\end{equation}
Furthermore, \cite{watson:Sra+Karp:2013} introduced a closed form Newton algorithm to solve \ref{kappasol}, given the assumption that  $g(\alpha, \beta, \kappa)$ can be easily evaluated
\begin{equation*} \label{newton}
\kappa_{n+1}= \kappa_{n}-\frac{g(\alpha,\beta,\kappa_{n})-r}{{g}'(\alpha,\beta,\kappa_{n})}=\kappa_{n}-\frac{g(\alpha,\beta,\kappa_{n})-r}{\left ( 1-\beta/\kappa_{n} \right )g\left ( \alpha,\beta,\kappa_{n} \right )+\left (\alpha/\kappa_{n} \right )-\left (g\left ( \alpha,\beta,\kappa_{n} \right )  \right )^{2}}.
\end{equation*}
Observe that the iteration can be performed only with one evaluation of the ratio $g(\alpha, \beta, \kappa)$.

This leads to the final contribution by \cite{watson::writeup1} where the authors have derived iterative bounds to evaluate $g\left ( \alpha,\beta,\kappa \right )$ for the cases where $0<\alpha<\beta$. The resulting bounds offer a cheap and efficient way to evaluate the neccesary function and show a fast convergence with asymptotically correct behavior. Additionally, it is demonstrated that already the first iteration of the bounds defines the same bounds as in  \cite{watson:Sra+Karp:2013} offering a better approximation for $g\left ( \alpha,\beta,\kappa \right )$ if more than one iteration is performed. This sugests to combine the techniques from  \cite{watson::writeup1} with the newton method derived by \cite{watson:Sra+Karp:2013} to solve the estimation problem \ref{kappasol}. 

What is more, since the first iterations are still defined in closed form, one can start the Newton procedure with the mid-value of the bounds and perform a bracketed type of Newton algorithm which combines derivative-based and bisection steps, with the starting brackets given by the bounds \ref{bounds}. With some small adjustments, this is also the to-go and default implementation in the package \pkg{watson}. 

A final possibility is to add to the whole procedure the logarithm and to solve
\begin{equation} \label{kappalogsol}
\log(g\left(\alpha,\beta, \kappa\right))=\log(r), \quad \text{where} \quad 0<\alpha<\beta, \text{ and } 0\leq r\leq 1.
\end{equation}
One can show that the newton step is then defined as
\begin{align} \label{lognewton}
\begin{split}
\kappa_{n+1}&= \kappa_{n}-\frac{\log(g(\alpha,\beta,\kappa_{n}))-\log(r)}{\log\left ({g}(\alpha,\beta,\kappa_{n})  \right )'}\\
&=\kappa_{n}-\frac{\log(g(\alpha,\beta,\kappa_{n}))-\log(r)}{\left ( 1-\beta/\kappa \right )+\left (\alpha/\left (\kappa_{n} g\left ( \alpha,\beta,\kappa_{n} \right ) \right ) \right )-g\left ( \alpha,\beta,\kappa_{n} \right ) },
\end{split}
\end{align}
again requiring only one evaluation of ${g}(\alpha,\beta,\kappa)$ per iteration. Following the results of \cite{watson::writeup1}, since it holds $$g(\alpha, \beta, \kappa) = 1 - g(\beta-\alpha, \beta, -\kappa), $$ w.l.o.g. one can assume $\kappa<0$, and thus the above method can numerically help in the cases where the previous derivative is numerically equal to zero, i.e., with extremly small or large values of $\kappa$. 

All of the mentioned cases are implemented in the \pkg{watson} package and the usage will be more discused in the section \ref{sec4}.

\section{Finite mixtures modeling}\label{sec3}

A finite mixtures modeling is a popular statistical tool in many research areas. These models allow to cluster observations by assuming that for each observation there exists a suitable parametric distribution, which defines a subgroup of the data. The mixture distribution is then a convex combination of the corresponding components, where the weight are usually specified by the affiliation of the data to given component.

The same way as on a line, also on spheres these models attracts a lot of popularity. The EM algorithm for the Watson distribution are for example given in  \cite{watson:Bijral+Breitenbach+Grudic:2007} and  \cite{watson:Sra+Karp:2013}. In the following, for the purpose of clarity, we will more stick to the latter, and hence the matrix notation.
\subsection{ Estimating the parameters of mixtures of Watson distributions}
Suppose we have $x_{1},\dots,x_{n}\in \mathbb{P}^{p-1}$ i.i.d. sample and as before define ${\bf X}$ to be the designe matrix with $x_{1},\dots,x_{n}$ as rows. We are interested in the modeling of the data into $K$ multivariate Watson distributions, that together form a mixture distribution. Let $W_{p}(x|\mu_{j},\kappa_{j})$ be the denisty of the $j$-th component, and $\pi_{j}$ is the corresponding weight. Then the density for a given observation $x_{d}$ is
\begin{equation*}
f(x_{d}|\mu_{1},\kappa_{1}\dots,\mu_{k},\kappa_{k})=\sum_{j=1}^{K}\pi_{j}W_{p}\left ( x_{d}|\mu_{j},\kappa_{j} \right ),
\end{equation*}
with a log-likelihood for the whole data given by
\begin{equation*}
\ell(x_{1},\dots,x_{n}|\mu_{1},\kappa_{1}\dots,\mu_{k},\kappa_{k})= \sum_{i=1}^{n}\log\left(\sum_{j=1}^{K}\pi_{j}W_{p}\left ( x_{i}|\mu_{j},\kappa_{j} \right )\right).
\end{equation*}

The EM algorithm for fitting mixtures of Watson distributions consists of the following steps:

\begin{enumerate}
\item Initialization: Assign the probabilities of the component memberships to each observation. This can  either be done by the user that starts the procedure or randomly. Such initialization allows further preprocessing as for example the diametrical clustering, which will be discussed later in this chapter. Finish the initialization using M-step which assigns the starting parameters of the mixture distribution using maximum likelihood techniques.

We note that an EM-algorithm is also possible to initialize with the given components parameters and continue with the E-step, however since  it is considered much easier to have a prior knowledge about the classification rather than the components parameters, this initialization is not offered in \pkg{watson}.

\item Repeat the following procedure until the convergence  or the maximum number of iterations is reached:
\begin{description}
  \item[E-step:] 
  First construct a lower bound for the log-likelihood $\ell(\cdot)$ given by
  \begin{equation*}
   \ell(x_{1},\dots,x_{n}|\mu_{1},\kappa_{1}\dots,\mu_{k},\kappa_{k})\geq \sum_{i,j}\beta_{ij} \log\frac{\pi_{j}W_{p}\left ( x_{i}|\mu_{j},\kappa_{j} \right )}{\beta_{ij} },
   \end{equation*}
   where $\beta_{ij}$ are the posterior defined as
   \begin{equation*}
  \beta_{ij}=\frac{\pi_{j}W_{p}\left ( x_{i}|\mu_{j},\kappa_{j} \right )}{\sum_{k=1}^{K}\pi_{k}W_{p}\left ( x_{i}|\mu_{k},\kappa_{k} \right )}.
   \end{equation*}
   This calculates the probabilities of belonging to a component conditional on the observed values, and defines the so-called soft-E-step. 
   
   From the numerical perspective it is safer to write this as 
   \begin{equation*}
  \log\beta_{ij}= \log\pi_{j}+\kappa_{j}(x_{i}^t\mu_{j})^2-\log M\left ( \frac{1}{2},\frac{p}{2},\kappa_{k} \right )-\log\left (  \sum_{k=1}^{K}\pi_{k}W_{p}\left ( x_{i}|\mu_{k},\kappa_{k} \right )\right )
   \end{equation*}
   and hence
   \begin{align*}
   \begin{split}
\log\beta_{ij}=&\log\pi_{j}+\kappa_{j}(x_{i}^t\mu_{j})^2-\log M\left ( \frac{1}{2},\frac{p}{2},\kappa_{k} \right )+m\\&-\log\left (  \sum_{k=1}^{K}\exp\left (\log\pi_{k}+\kappa_{j}(x_{k}^t\mu_{k})^2-\log M\left ( \frac{1}{2},\frac{p}{2},\kappa_{k} \right )-m  \right )\right ),
   \end{split}
   \end{align*}
   where $m = \argmax\limits_{j} \ \log\pi_{j} + \log W_{p}\left ( x_{i}|\mu_{j},\kappa_{j} \right )$.
   
   Further possibility is to use an hard assignment step (see, \cite{watson:Sra+Karp:2013}), also called categorical step, where:
   \begin{equation*}
    \beta_{ij}=\left\{\begin{matrix}
1 & \text{if } &j = \argmax\limits_{j'} \ \log\pi_{j'} + \log W_{p}\left ( x_{i}|\mu_{j'},\kappa_{j'} \right ), \\ 
0 & \text{else}.&
\end{matrix}\right.
   \end{equation*}
   If the maximum is not unique, the category is assigned randomly to one of the leading categories. 
   
   A final choice is to use the so called stochastic step \cite{watson:Celeux}, or S-step, where the category is assigned at random for each observation $i$ to one component $j$, with probability equal to its posterior probability $\beta_{i,j}$.
  \item[M-step:] The M-step is defined by the  maximization of the expected log-likelihood by determining separately
for each cluster $j$, the optimal parameters $\mu_{j}$ and $\kappa_{j}$, i.e.,:
   \begin{equation*}
   \mathbf{S}^{j}=\frac{\sum_{i=1}^{n}\beta_{ij}x_{i}x_{i}^{t}}{\sum_{i=1}^{n}\beta_{ij}},\qquad \pi_{j}=\frac{1}{n}\sum_{i=1}^{n}\beta_{ij} ,    \qquad \kappa_{j}=g^{-1}\left ( 0.5,p/2, \mu_{j}^{t}\mathbf{S}^{j}\mu_{j} \right )
   \end{equation*}
   \begin{equation*}
   \mu_{j}=s_{p}^{j} \quad \text{if } \kappa_{j}>0, \qquad \mu_{j}=s_{1}^{j} \quad \text{if } \kappa_{j}<0,
   \end{equation*}
   where $s_{1}^{j},\dots,s_{p}^{j}$ are the eigenvectors of $\mathbf{S}^{j}$ ordered such that the appertained eigenvalues satisfy $\lambda_{1}^{j}\leq\lambda_{2}^{j}\leq\dots\leq \lambda_{p}^{j}$.
  \item[Convergence check:] Stop if the if the relative absolute change in the log-likelihood is smaller than a given threshold. Note that the calculation of log-likelihood is strongly connected with the E-step calculations, and thus the convergence is assessed during this procedure. 
     
\end{description}
\end{enumerate}

\subsection{Diametrical clustering}
The directional clustering and more precisely the algorithm proposed in \cite{watson:dhillon:2003} is a well known clustering technique in bioinformatics. The algorithm uses a non-parametric method and similarly as the EM algorithm, it is based on first picking the cateogry that maximizes the squared scalar product  (E-step), and then defining new concentration directions for each cluster (M-step). 

\cite{watson:Sra+Karp:2013} showed that this algorithm is equivalent to the Watson EM algorithm with the $\kappa_{j}\rightarrow\infty,$ $\forall j=1,\dots,K$, which implies that the $\beta_{i,j}\rightarrow \{0,1\}$. Equivalently, the authors showed that this can be also seen as a hard-assignment EM algorithm where all $\kappa$ parameters are equal and hence can be ignored. This suggests that it is natural to expect a better performence with the Watson mixture modeling (as the diametrical clustering can be seen as a special case), however thanks to the robustness of the algorithm one may still use the diametrical clustering in the initialization phase as a preprocessing before the main part of the EM starts.

The watson package offers such preprocesing of the model using the \code{init\_iter} parameter in the \fun{watson()} function, which will be more discused in the next chapter. Alternativelly the user may also directly apply only directional clustering to the data by using the \fun{diam\_clus()} function.

<<echo=FALSE>>=
cat("R> diam_clus(x, k, niter = 100)")
@ 
%
The arguments for this function are as follows.
\begin{description}
\item[\normalfont \code{x}:] A numeric data matrix, with rows corresponding to observations. Can be a dense matrix,
      or any of the supported sparse matrices from \pkg{Matrix} package by \pkg{Rcpp}.
\item[\normalfont \code{k}:] An integer indicating the number of components.
\item[\code{niter}:] Integer indicating the number of
   iterations of the diametrical clustering algorithm. (Default: 100.)
\end{description}



\subsection{Illustrative example}





\section{Software}\label{sec4}

The main function of the \pkg{watson} package is the \fun{watson()} function for fitting mixtures of Watson
distributions. It can be called as:
%
<<echo=FALSE>>=
cat("R> watson(x, k, control = list(), ...)")
@ 
%
The arguments for this function are as follows.
\begin{description}
\item[\normalfont \code{x}:] A numeric data matrix, with rows corresponding to observations. Can be a dense matrix,
      or any of the supported sparse matrices from \pkg{Matrix} package by \pkg{Rcpp}.
\item[\normalfont \code{k}:] An integer indicating the number of components.
\item[\normalfont \code{control}:] A list of control parameters consisting of
 \begin{description}
  \item[\code{M}:] This argument allows to specify how to estimate
   the concentration parameters during the M-step:
   The method for solving for the concentration parameters can
     be \mbox{specified} by one of \code{"Sra\_2007"},
     \code{"BBG"}, \code{"BBG\_c"},
     \code{"Sra\_Karp\_2013"}, \code{"bisection"}, \code{"lognewton"} and
     \code{"newton"} (default). For more details on these methods see
     Section~\ref{sec2}.
   \item[\code{E}:] Specifies the transformation of posterior probabilities during the E-step. Possible values \code{"softmax"} (default), \code{"hardmax"}, and \code{"stochmax"}.For more details on these methods see
     Section~\ref{sec3}.
 \item[\code{converge}:] Logical indicating if convergence of the
   algorithm should be checked and stopped if the relative change in likelihood is smaller than \code{reltol}. 
   In case \code{converge = FALSE}, all iterations are performed and the one with maximum archieved likelihood
   is returned. This makes mostly sense and is also the default case if  \code{E} is equal to \code{"softmax"}.  
  \item[\code{maxiter}:] Integer indicating the maximum number of
   iterations of the EM algorithm. (Default: 100.)
  \item[\code{reltol}:] Relative change threshold. Ignored if \code{converge = FALSE}.
   (Default: \verb|sqrt(.Machine$double.eps)|.)
  \item[\code{ids}:] Indicates either the class memberships of the
    observations or if equal to \code{TRUE} the class memberships are
    obtained from the attributes of the data. In this way the class
    memberships are for example stored if data is generated using
    function \code{rmwat()}. If this argument is specified, the EM
    algorithm is stopped after initialization, i.e., the parameter
    estimates are determined conditional on the known true class
    memberships (one M-step is performed).
   \item[\code{init\_iter}:] 
   a numeric vector setting the number of diametrical clustering iterations to do, before the EM starts, default: 0.
  \item[\code{start}:]  A specification of the starting values to be employed. Can be a list of matrices giving the memberships
   of objects to components. This information is combined with the \code{init\_iter} parameter and together form
   the initialization procedure. If nothing is given, the starting values are drwan randomly.
   
   If several starting values are specified, the EM algorithm is performed individually to each starting 
   value, and the best solution found is returned.
  \item[\code{nruns}:] An integer giving the number of EM runs to be performed. Default: 1. Only used if \code{start} or \code{ids} is not given.
  \item[\code{minweight}:]  A numeric indicating the minimum prior probability. Components falling below this threshold are removed
   during the iteration. If ≥ 1, this is taken as the minimal number of observations in a component, default: 0 if
  \code{E = "softmax"} and 2 if other type of E-method is used .
  \item[\code{N}:]  An integer indicating number of iteration used when the Kummer function and its ratio are approximated, default: 30.
  \end{description}
\end{description}


The object returned by \fun{watson()} is of the \class{watfit} class with methods \fun{print}, \fun{coef}, \fun{logLik} and
\fun{predict} (yields either the component assignments or the matrix of a-posteriori probabilities). 


\section{Numerical issues} \label{sec5}

In the following we will use the notation from \ref{sec2} and write 
\begin{equation*}
g(\alpha, \beta, z) = \frac{\mathrm{d} \log(M(\alpha, \beta, z)))}{\mathrm{d} z} =  \frac{{M(\alpha, \beta, z)}'}{M(\alpha, \beta, z)} = \frac{\alpha}{\beta}\frac{{M(\alpha+1, \beta+1, z)}}{M(\alpha, \beta, z)}.
\end{equation*}
As shown in Section \ref{sec2} and \ref{sec3}, computing ML estimation of concentration parameters for Watson distributions on $\mathbb{R}^p$ necessitates the solution $\kappa$ of
$$g\left(\alpha,\beta, \kappa\right)=r, \quad \text{where} \quad 0<\alpha<\beta, \text{ and } 0\leq r\leq 1.$$
Furthermore a computation of log-likelihoods or the a-posteriori probabilities in the mixture modeling requires to evaluate the expressions as 
$\log(M(\alpha, \beta, z))$, where $0<\alpha<\beta$. Because $M(\alpha, \beta, z)\rightarrow\infty$ as $z\rightarrow\infty$ a direct evaluation of $M(\cdot)$ with further logarithm or quotient function aplication is clearly not a good idea, mostly if we know that the fraction satsfies $0<g(\alpha, \beta, z)<1$. In general it can be easily observe that the Kummer's function easily over- or underflow for very general set of parameters just because of the geometric series structure the function embraces. 

\subsection{Approximation of the Kummer's ratio}

We assume $\beta>\alpha>0,$ $z<0$ and follow the approach used in \cite{watson::writeup1}. Define the sequences of real numbers through the recursive relation as 
\begin{equation*} 
l_{n}^{(0)}(z)=l_{\alpha+n,\beta+n}^{(0)}(z)=\frac{2\left (\alpha +n \right )}{\sqrt{(z-\left ( \beta +n \right ))^2 + 4 \left (\alpha+n  \right ) z} -z + \left (\beta +n \right )},
\end{equation*}
\begin{equation*} 
l_{n}^{(m)}(z)=\frac{\alpha+n}{\beta+n-z+zl_{n+1}^{(m-1)}(z)},
\end{equation*}
and 
\begin{equation*} 
u_{n}^{(0)}(z)=u_{\alpha+n,\beta+n}^{(0)}(z)= 1 - \frac{2\left ( \beta -\alpha  \right )}{\sqrt{\left ( z+\left (\beta +n \right )+1 \right )^{2}-4(\beta -\alpha+1 )z}+z+\left (\beta+n  \right )-1},
\end{equation*}
\begin{equation*} 
u_{n}^{(m)}(z)=\frac{\alpha+n}{\beta+n-z+zu_{n+1}^{(m-1)}(z)}.
\end{equation*}
The sequences $\left(l_{0}^{(0)}(z),l_{0}^{(1)}(z),l_{0}^{(2)}(z),\dots \right)$ and  $\left(u_{0}^{(0)}(z),u_{0}^{(1)}(z),u_{0}^{(2)}(z),\dots \right)$  converge monotonically to $g(\alpha,\beta,z)$  from below and above respectively. For the proof see \cite{watson::writeup1}. 

Using another result of \cite{watson::writeup1}, i.e., $g(\alpha, \beta, z) = 1 - g(\beta-\alpha, \beta, -z),$ the evaluation routine can be easily described by the algorithm \ref{A1}, where the approximation function calculates $l_{0}^{(N)}(z)$ and $u_{0}^{(N)}(z)$ and returns the middle value in between.
\begin{algorithm}
\label{A1}
\caption{Kummer's’s function algorithm}
\begin{algorithmic}[1]
\Procedure{Kummer's}{$\alpha,\beta,z,N$ = number of iterations}
\If {$z =0$} \Return $\alpha/\beta$
\EndIf
\If {$z <0$} g = approximate($\alpha,\beta,z,N$)
\State \Return $g$
\EndIf
\If {$z>0$} g = approximate($\beta-\alpha,\beta,-z,N$)
\State \Return $1-g$
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

The convergence  of the irrational bounds, for the values $\alpha = 0.5,$ $\beta=50$ and $\alpha=99.5,$ $\beta=100$ is visualized in the following figures. Note that the parameters were specifically chosen to be either close or far away from each other as this makes the approximation more difficult, because the crossing point $z=0$, where the bounds are exact, is either after or before the steepest part of the function respectively. The ``true" values were calculated using Mathematica software for higher precision.

\noindent
<<echo=FALSE,message=FALSE,fig.height=6.5,fig.width=10, cache=TRUE>>=
library(ggplot2)
library(mistr)
iratio_gneg <- function(a,b,z,N,change=FALSE){
   start1 <- (2*(a+N))/(b+N-z+sqrt((z-b-N)^2+4*(a+N)*z)) 
   start2 <- 1- (2*(b-a))/((b+N)-1 + z + sqrt(-4*((b-a)+1)*z + (z + (b+N+1))^2 ) )
   s <- c(start1,start2)
   if(N>0){
      for(i in (N-1):0){
      s <- (a+i)/((b+i) - z + z * s)
      }
   }
   if(change) s=1-s
   data.frame(y=s,type= as.factor(rep(c("l","u"),each =length(z))))
}
g <- function(a,b,z,N){
   m <- data.frame(y = numeric(length(z)*2), type = factor(c("l","u")))
   z2 <- rep(z,2)
   m[z2==0,] <- data.frame(y=rep(a/b, sum(z2==0)),type = factor(rep(c("l","u"),each = sum(z==0))))
   m[z2<0,] <- iratio_gneg(a,b,z[z<0],N)
   m[z2>0,] <- iratio_gneg(b-a,b,-z[z>0],N,change = TRUE)
   m
}
x = seq(-100,200,0.1)
a=0.5
b=50
N=3
d<- do.call("rbind", lapply(c(0,1,5), function(i) data.frame(x =x, g(a,b,x,i))))
d <- cbind(d, N= as.factor(rep(c(0,1,5), each = 2*length(x))))
a05b50 = read.csv(file = "~/Dropbox/Watson/a05b50",header = FALSE)
colnames(a05b50) <- "y"
d2 <- cbind(x,a05b50, type = as.factor("l"), N = as.factor("true"))
d3 <- rbind(d,d2)
# d <- rbind(d,cbind(data.frame(x =x, g(a,b,x,30)), N= as.factor(rep(30, each = 2*length(x)))))
p1 =ggplot(d3)+
   # geom_line(data = d2, aes(x=x, y=V1), size = 1.8, alpha=0.9, color = "#f28df9")+
   geom_line(aes(x,y,col=N, lty = type), size = 0.8) +
   mistr_theme(legend.position = "top")+
   ggplot2::labs(x = NULL, y = NULL, title = expression("g"~(alpha~"=0.5,"~beta~"=50")))

d<- do.call("rbind", lapply(c(0,1,5,10,30), function(i) data.frame(x =x, g(a,b,x,i))))
d <- cbind(d, N= as.factor(rep(c(0,1,5,10,30), each = 2*length(x))))
d$y <- d$y-a05b50$y
p2 =ggplot(d)+
   # geom_line(data = d2, aes(x=x, y=V1), size = 1.8, alpha=0.9, color = "#f28df9")+
   geom_line(aes(x,y,col=N, lty = type), size = 0.8) +
   mistr_theme(legend.position = "top")+
   ggplot2::labs(x = NULL, y = NULL, title = expression("g"~(alpha~"=0.5,"~beta~"=50")~"-differences"))


   
x = seq(-500,100,0.1)
a=99.5
b=100
N=50
d<- do.call("rbind", lapply(c(0,1,5), function(i) data.frame(x =x, g(a,b,x,i))))
d <- cbind(d, N= as.factor(rep(c(0,1,5), each = 2*length(x))))
a995b100 = read.csv(file = "~/Dropbox/Watson/a995b100",header = FALSE)
colnames(a995b100) <- "y"
d2 <- cbind(x,a995b100, type = as.factor("l"), N = as.factor("true"))
d3 <- rbind(d,d2)
p3=ggplot(d3)+
   # geom_line(data = d2, aes(x=x, y=V1), size = 1.8, alpha=0.9, color = "#f28df9")+
   geom_line(aes(x,y,col=N, lty = type), size = 0.8) +
   mistr_theme(legend.position = "none")+
   ggplot2::labs(x = NULL, y = NULL, title = expression("g"~(alpha~"=99.5,"~beta~"=100")))

d<- do.call("rbind", lapply(c(0,1,5,10,30), function(i) data.frame(x =x, g(a,b,x,i))))
d <- cbind(d, N= as.factor(rep(c(0,1,5,10,30), each = 2*length(x))))
d$y <- d$y-a995b100$y
p4=ggplot(d)+
   # geom_line(data = d2, aes(x=x, y=V1), size = 1.8, alpha=0.9, color = "#f28df9")+
   geom_line(aes(x,y,col=N, lty = type), size = 0.8) +
   mistr_theme(legend.position = "none")+
   ggplot2::labs(x = NULL, y = NULL, title=expression("g"~(alpha~"=99.5,"~beta~"=100")~"-differences"))
mistr:::multiplot(p1,p3,p2,p4,cols=2)
@
The plot with the differences indicates that even with the chosen parameters after 5 iteration the irrational bounds are decently close and after 30 iterations almost exact. Furthermore it should be pointed out that the iterations are composed only by four very simple arithmetic operations, thus in a compiled programming languages as for example C or C++ such estimation even for big $N$ requires almost no time.

To solve the inverse of $g(\alpha, \beta, z)$ one combines this evaluation with a bracketed type of Newton algorithm from section \ref{sec2}. Note that $$u_{\alpha,\beta}^{(0)^{-1}}(r)=L(r), \quad u_{\alpha,\beta}^{(1)^{-1}}(r)=K(r), \quad l_{\alpha,\beta}^{(1)^{-1}}(r)=U(r)$$ (see Theorem~6 of \cite{watson::writeup1}) offers exactly such brackets for the starting value. 

\subsection{Approximation of the Kummer's function logarithm}

Since the $g(\alpha,\beta,z)$ is essentially the logarithmic derivative of the Kummer's function $M(\alpha,\beta,z)$ it can be also directly use to calculate its logarithm. For the following let $m(\alpha,\beta,z) = \log(M(\alpha,\beta,z))$ and $0<\alpha<\beta$. 

First thing that can be done it for a specific value $z$, evaluate the $m(\alpha,\beta,x)$ at some value $x$ and the rest of the difference can be obtained using numerical integration of $g(\alpha,\beta,z)$, whose cheap and effective evaluation was describe in the previous section. Using the fact that $M(\alpha,\beta,0)=1$ $\forall \alpha,\beta$ the general formula can be written as
\begin{equation*}
m(\alpha,\beta,z) = m(\alpha,\beta,0) + \int_{0}^{z}g(\alpha,\beta,z) dz= \int_{0}^{z}g(\alpha,\beta,z) dz. 
\end{equation*}
Equivalently one may choose a value $x$ for which the value of $M(\alpha,\beta,x)$ evaluated in some mathematical libraries (like "gsl") still does not underflow or overflow and integrate only the rest. 

Another approach is to use the properties of $g(\alpha,\beta,z)$ and rewrite $m(\alpha,\beta,z)$ as a sum of parts which form it. This would still require one evaluation of the Kummer's function, but one can choose the parameters for which $M(\alpha,\beta,z)$ either does not underflow or overflow. An example of such case is e.g. $M(\alpha,\beta,z)$ where $\alpha<1$ and $\beta<2$ and $z$ is negative. For such a set of parameters the evaluation using gsl library does not underflow even for cases as $z=-1\times 10^{300}$ and additionally the approximation error is negliable. This behavior is presented because the raising factorials of $M(\alpha,\beta,z)$ do not increase fast enough. For the following recall that $\alpha=0.5$, the deviation for a more general case can be found in  \cite{watson::writeup1}. Formally,
\begin{equation*}
m(\alpha,\beta,z)= z+m(\beta-\alpha,\beta,-z),
\end{equation*}
where we applied the Kummer's identity $M(\alpha,\beta,z)=e^{z}M(\beta-\alpha,\beta,-z)$. Thus 
\begin{align*}
\begin{split}
m(\alpha,\beta,z)=&z +\log(g(\beta-\alpha-1,\beta-1,-z))+\log(\beta-1)-\log(\beta-\alpha-1)\\
+&m(\beta-\alpha-1,\beta-1,-z). 
\end{split}
\end{align*}
Recursive rewriting the above expression yields
\begin{align*}
\begin{split}
=z&+\underbrace{\sum_{i=1}^{\lfloor \beta-\alpha\rfloor_{s}} \left ( \log(g(\beta-\alpha-i,\beta-i,-z))+\log(\beta-i)-\log(\beta-\alpha-i) \right )}_{S}\\
&+m(\beta-\alpha-\lfloor \beta-\alpha\rfloor_{s},\beta-\lfloor \beta-\alpha\rfloor_{s},-z)
\end{split}
\end{align*}
where $\lfloor x\rfloor_{s}=\max_{m\in\mathbb{Z}}m<x$ is the strict floor operator. Hence
\begin{equation*}
m(\alpha,\beta,z)=\left\{\begin{matrix}
S+z+m(\beta-\alpha-\lfloor \beta-\alpha\rfloor_{s},\beta-\lfloor \beta-\alpha\rfloor_{s},-z), & \text{if } z>0, \\ 
S+m\left (\alpha,\beta-\lfloor \beta-\alpha\rfloor_{s},z  \right ), & \text{if }z<0,\end{matrix}\right.
\end{equation*}
where we again applied the Kummer's identity for the negative case. A performence of such procedure is visualised in  \cite{watson::writeup1}.

This method is also implemented in the \pkg{watson} as the last case scenario. First using the algorithms and asymtoticall formulas implemented in ``gsl - scientific library" the code tryies to evaluate the function directly. If the resulting error value is not 0, (i.e. GSL\_SUCCESS), the code triest to evaluate the transformed version using the Kummer's identity. If also this method fails (this is usually only for very big values, or parameters), the above method is used which has not been observed to fail so far.  

\section{Simulation study}

\bibliographystyle{plainnat}
\bibliography{watson.bib}

\end{document}
